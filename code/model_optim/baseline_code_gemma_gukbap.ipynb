{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-425</td>\n",
       "      <td>상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...</td>\n",
       "      <td>상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?</td>\n",
       "      <td>[ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-426</td>\n",
       "      <td>(가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...</td>\n",
       "      <td>(가)에 대한 설명으로 옳지 않은 것은?</td>\n",
       "      <td>[고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-427</td>\n",
       "      <td>나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...</td>\n",
       "      <td>(가) 지역에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-428</td>\n",
       "      <td>이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....</td>\n",
       "      <td>밑줄 친 ‘그’에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-429</td>\n",
       "      <td>선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...</td>\n",
       "      <td>(가) 인물이 추진한 정책으로 옳지 않은 것은?</td>\n",
       "      <td>[사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                          paragraph  \\\n",
       "0  generation-for-nlp-425  상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...   \n",
       "1  generation-for-nlp-426  (가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...   \n",
       "2  generation-for-nlp-427  나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...   \n",
       "3  generation-for-nlp-428  이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....   \n",
       "4  generation-for-nlp-429  선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...   \n",
       "\n",
       "                                question  \\\n",
       "0  상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?   \n",
       "1                 (가)에 대한 설명으로 옳지 않은 것은?   \n",
       "2                 (가) 지역에 대한 설명으로 옳은 것은?   \n",
       "3               밑줄 친 ‘그’에 대한 설명으로 옳은 것은?   \n",
       "4             (가) 인물이 추진한 정책으로 옳지 않은 것은?   \n",
       "\n",
       "                                             choices  answer question_plus  \n",
       "0                           [ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]       2          None  \n",
       "1  [고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...       1          None  \n",
       "2  [이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...       4          None  \n",
       "3  [살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...       2          None  \n",
       "4  [사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....       3          None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "id                  0\n",
      "paragraph           0\n",
      "question            0\n",
      "choices             0\n",
      "answer              0\n",
      "question_plus    2031\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2031 non-null   object\n",
      " 1   paragraph      2031 non-null   object\n",
      " 2   question       2031 non-null   object\n",
      " 3   choices        2031 non-null   object\n",
      " 4   answer         2031 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 95.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF\n",
    "\n",
    "- TF-IDF 참고 링크: https://ko.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_question\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_question'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTF-IDF Features:\")\n",
    "display(tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `assignment_2_persona` has been saved to /data/ephemeral/home/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `assignment_2_persona`\n"
     ]
    }
   ],
   "source": [
    "# 본인의 Huggingface auth token 입력\n",
    "## Jupyter lab에서 로그인 하는 textbox가 나오지 않을 경우, terminal에서 로그인 하실 수 있습니다.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델과 토크나이저를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_111291/676208445.py\", line 11, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4097, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 963, in __init__\n",
      "    super().__init__(config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1432, in __init__\n",
      "    self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 1235, in from_model_config\n",
      "    generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 1093, in from_dict\n",
      "    config = cls(**{**config_dict, **kwargs})\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 475, in __init__\n",
      "    self.validate(is_init=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 751, in validate\n",
      "    logger.warning_once(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 328, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4afd0c114d34504b5650780207b9355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HumanF-MarkrAI/Gukbap-Gemma2-9B\",\n",
    "    torch_dtype=torch.float16, \n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    "    use_cache=False\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HumanF-MarkrAI/Gukbap-Gemma2-9B\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gemma-ko-2b 모델에는 chat template 이 없기 때문에 직접 입력해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\\n' + content + '<end_of_turn>\\n<start_of_turn>model\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<end_of_turn>\\n' }}{% endif %}{% endfor %}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus', 'full_question', 'question_length'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-425',\n",
       " 'messages': [{'role': 'system', 'content': '지문을 읽고 질문의 답을 구하세요.'},\n",
       "  {'role': 'user',\n",
       "   'content': '지문:\\n상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服) 절차에 대하여 논한 것이 신과는 큰 차이가 있었습니다 . 장자를 위하여 3년을 입는 까닭은 위로 ‘정체(正體)’가 되기 때문이고 또 전 중(傳重: 조상의 제사나 가문의 법통을 전함)하기 때문입니다 . …(중략) … 무엇보다 중요한 것은 할아버지와 아버지의 뒤를 이은 ‘정체’이지, 꼭 첫째이기 때문에 참 최 3년 복을 입는 것은 아닙니다 .”라고 하였다 .－현종실록 －ㄱ.기 사환국으로 정권을 장악하였다 .ㄴ.인 조반정을 주도 하여 집권세력이 되었다 .ㄷ.정조 시기에 탕평 정치의 한 축을 이루었다 .ㄹ.이 이와 성혼의 문인을 중심으로 형성되었다.\\n\\n질문:\\n상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?\\n\\n선택지:\\n1 - ㄱ, ㄴ\\n2 - ㄱ, ㄷ\\n3 - ㄴ, ㄹ\\n4 - ㄷ, ㄹ\\n\\n1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\\n정답:'},\n",
       "  {'role': 'assistant', 'content': '2'}],\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4025dfde1a437b949af176d6e11178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7064acda9d1344de9729cbadbbf00ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26ea5bd1b964e438db2decce1dabef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지문을 읽고 질문의 답을 구하세요.user\n",
      "지문:\n",
      "홍콩 정부가 10일로 예정됐던 시위대와의 대화를 전격 취소한 가운데 렁춘잉 행정장관(사진)이 뇌물 수수 혐의로 사정 당국의 조사를 받게 됐다. 2017년 실시되는 행정장관 선출 방식을 계기로 촉발된 홍콩의 시위 사태가 갈수록 복잡하게 전개되는 모습이다.사우스차이나모닝포스트(SCMP)는 이날 렁 장관이 호주 기업으로부터 거액의 자금을 받고도 신고하지 않았다는 의혹으로 홍콩 반(反)부패 당국의 조사를 받을 예정이라고 보도했다. 림스키 위안 홍콩 법무장관은 야당인 신민주동맹이 반부패 수사기구인 부패방지위원회에 렁 장관을 고발한 것과 관련해 편파수사 의혹을 피하기 위해 케이스 영 검찰총장에게 수사 전권을 위임했다고 밝혔다.앞서 호주 일간지 디에이지는 지난 8일 렁 장관이 호주 기업으로부터 2012년과 2013년 두 차례에 걸쳐 모두 400만파운드(약 69억원)를 받고 신고하지 않은 의혹이 있다고 보도했다. 이에 대해 행정장관실은 렁 장관이 과거 부동산 컨설팅 회사에 다닐 때 호주 기업에 제공한 자문서비스와 관련해 대금을 받은 것으로, 취임하기 전 대금 지급계약이 이뤄졌기 때문에 신고할 필요가 없었다고 해명했다. 하지만 홍콩 야당 의원들은 뇌물수수법 위반 의혹을 제기하며 렁 장관의 탄핵 절차에 들어가겠다고 경고했고, 일부 의원은 그를 부패방지위원회에 고발했다.\n",
      "\n",
      "질문:\n",
      "렁춘잉 행정장관이 뇌물 수수 혐의로 조사를 받게 된 이유는 무엇인가?\n",
      "\n",
      "선택지:\n",
      "1 - 호주 기업으로부터 거액의 자금을 받고 신고하지 않음\n",
      "2 - 시위대와의 대화를 전격 취소함\n",
      "3 - 부동산 컨설팅 회사에서 일한 경력\n",
      "4 - 행정장관 선출 방식의 변경\n",
      "5 - 부패방지위원회에 고발당함\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "정답:\n",
      "model\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "# vram memory 제약으로 인해 인풋 데이터의 길이가 1024 초과인 데이터는 제외하였습니다. *힌트: 1024보다 길이가 더 긴 데이터를 포함하면 더 높은 점수를 달성할 수 있을 것 같습니다!\n",
    "# tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 1024)  \n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "def shift_labels(example):\n",
    "    input_ids = example['input_ids']\n",
    "    labels = input_ids[1:] + [tokenizer.pad_token_id]\n",
    "    assert len(input_ids) == len(labels), \"input_ids와 labels의 길이가 맞지 않습니다.\"\n",
    "    example['labels'] = labels\n",
    "    return example\n",
    "\n",
    "# # 데이터셋에 시프트된 labels 추가\n",
    "train_dataset = train_dataset.map(shift_labels)\n",
    "eval_dataset = eval_dataset.map(shift_labels)\n",
    "\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1696\n",
      "min token length: 120\n",
      "avg token length: 688.8226600985222\n"
     ]
    }
   ],
   "source": [
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\n",
      "' + content + '<end_of_turn>\n",
      "<start_of_turn>model\n",
      "' }}{% elif message['role'] == 'assistant' %}{{ content + '<end_of_turn>\n",
      "' }}{% endif %}{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion 부분만 학습하기 위한 data collator 설정\n",
    "\n",
    "- 텍스트 중 response_template 까지는 ignore_index 로 loss 계산에서 제외\n",
    "- 텍스트 중 response_template 이후는 학습에 포함 (정답 + eos 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"<start_of_turn>model\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 logits 를 조정하여 정답 토큰 부분만 출력하도록 설정\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric 로드\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 정답 토큰 매핑\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "\n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = list(map(lambda x: x.split(\"<end_of_turn>\")[0].strip(), labels))\n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그로 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<bos>',\n",
       " 'eos_token': '<eos>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<eos>',\n",
       " 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad token 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA를 사용할 수 없습니다. CPU로 모델을 학습합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # 예: 초기 레이어를 동결\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1624\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    gradient_accumulation_steps=32,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length=1024,\n",
    "    output_dir=\"outputs_gemma\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5, \n",
    "    gradient_checkpointing=True,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\", \n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/150 27:33 < 1:25:44, 0.02 it/s, Epoch 0.73/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2237\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de981a6f2fb4e8684fdc348d7d310ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "# TODO 학습된 Checkpoint 경로 입력\n",
    "checkpoint_path = \"../../data/outputs_gemma/checkpoint-126\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data 경로 입력\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <보기>가 있을 때\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [38:40<00:00,  2.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 14s, sys: 9min 24s, total: 38min 39s\n",
      "Wall time: 38min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if hasattr(model, 'module'):\n",
    "    model.module = model.module.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "    \n",
    "if hasattr(model, 'embed_tokens'):\n",
    "    model.embed_tokens = model.embed_tokens.to(device)\n",
    "\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()  # CUDA 캐시 정리\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Check if inputs is a dict or tensor\n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)  \n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "infer_results = []\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).half()\n",
    "\n",
    "# 모델의 forward 함수를 체크포인트로 감싸기\n",
    "def forward_with_checkpoint(input):\n",
    "    return checkpoint(model, input)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 배치 크기 설정 (메모리 사용량에 따라 조정)\n",
    "batch_size = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        \n",
    "        for data in batch:\n",
    "            _id = data[\"id\"]\n",
    "            messages = data[\"messages\"]\n",
    "            len_choices = data[\"len_choices\"]\n",
    "            \n",
    "            try:\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # CPU에서 처리\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                outputs = forward_with_checkpoint(inputs)\n",
    "                logits = outputs.logits[:, -1].flatten()\n",
    "\n",
    "                target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "                probs = torch.nn.functional.softmax(\n",
    "                    torch.tensor(target_logit_list, dtype=torch.float32),\n",
    "                    dim=-1\n",
    "                ).numpy()\n",
    "\n",
    "                predict_value = pred_choices_map[np.argmax(probs)]\n",
    "                infer_results.append({\"id\": _id, \"answer\": predict_value})\n",
    "                \n",
    "                print(f\"Processed sample {_id}: predicted {predict_value}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # 메모리 정리\n",
    "                del inputs\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"Processed {len(infer_results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
