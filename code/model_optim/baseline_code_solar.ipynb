{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, EarlyStoppingCallback, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-425</td>\n",
       "      <td>상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...</td>\n",
       "      <td>상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?</td>\n",
       "      <td>[ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-426</td>\n",
       "      <td>(가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...</td>\n",
       "      <td>(가)에 대한 설명으로 옳지 않은 것은?</td>\n",
       "      <td>[고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-427</td>\n",
       "      <td>나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...</td>\n",
       "      <td>(가) 지역에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-428</td>\n",
       "      <td>이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....</td>\n",
       "      <td>밑줄 친 ‘그’에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-429</td>\n",
       "      <td>선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...</td>\n",
       "      <td>(가) 인물이 추진한 정책으로 옳지 않은 것은?</td>\n",
       "      <td>[사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                          paragraph  \\\n",
       "0  generation-for-nlp-425  상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...   \n",
       "1  generation-for-nlp-426  (가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...   \n",
       "2  generation-for-nlp-427  나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...   \n",
       "3  generation-for-nlp-428  이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....   \n",
       "4  generation-for-nlp-429  선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...   \n",
       "\n",
       "                                question  \\\n",
       "0  상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?   \n",
       "1                 (가)에 대한 설명으로 옳지 않은 것은?   \n",
       "2                 (가) 지역에 대한 설명으로 옳은 것은?   \n",
       "3               밑줄 친 ‘그’에 대한 설명으로 옳은 것은?   \n",
       "4             (가) 인물이 추진한 정책으로 옳지 않은 것은?   \n",
       "\n",
       "                                             choices  answer question_plus  \n",
       "0                           [ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]       2          None  \n",
       "1  [고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...       1          None  \n",
       "2  [이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...       4          None  \n",
       "3  [살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...       2          None  \n",
       "4  [사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....       3          None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "id                  0\n",
      "paragraph           0\n",
      "question            0\n",
      "choices             0\n",
      "answer              0\n",
      "question_plus    2031\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2031 non-null   object\n",
      " 1   paragraph      2031 non-null   object\n",
      " 2   question       2031 non-null   object\n",
      " 3   choices        2031 non-null   object\n",
      " 4   answer         2031 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 95.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF\n",
    "\n",
    "- TF-IDF 참고 링크: https://ko.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_question'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTF-IDF Features:\")\n",
    "display(tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `assignment_2_persona` has been saved to /data/ephemeral/home/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `assignment_2_persona`\n"
     ]
    }
   ],
   "source": [
    "# 본인의 Huggingface auth token 입력\n",
    "## Jupyter lab에서 로그인 하는 textbox가 나오지 않을 경우, terminal에서 로그인 하실 수 있습니다.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델과 토크나이저를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa20b4595e05488c85c793dea94cc736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 답을 말해주세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    ) \n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리\n",
    "# vram memory 제약으로 인해 인풋 데이터의 길이가 1024 초과인 데이터는 제외하였습니다. *힌트: 1024보다 길이가 더 긴 데이터를 포함하면 더 높은 점수를 달성할 수 있을 것 같습니다! \n",
    "# filtered_tokenized_dataset = tokenized_dataset.map(filter_dataset_with_bm25) \n",
    "# final_tokenized_dataset = filter_dataset_with_bm25(filtered_tokenized_dataset)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 1024)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# eos 토큰 추가하기 \n",
    "# def add_eos_token(example):\n",
    "#     example['input_ids'].append(tokenizer.eos_token_id)\n",
    "#     example['attention_mask'].append(1)\n",
    "#     return example\n",
    "\n",
    "# train_dataset = train_dataset.map(add_eos_token)\n",
    "# eval_dataset = eval_dataset.map(add_eos_token)\n",
    "\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.column_names)\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion 부분만 학습하기 위한 data collator 설정\n",
    "\n",
    "- 텍스트 중 response_template 까지는 ignore_index 로 loss 계산에서 제외\n",
    "- 텍스트 중 response_template 이후는 학습에 포함 (정답 + eos 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(features, tokenizer, response_template):\n",
    "    for feature in features:\n",
    "        print(f\"Input length: {len(feature['input_ids'])}\")\n",
    "        input_ids = feature['input_ids']\n",
    "        labels = [-100] * len(input_ids)  # Initialize all labels as ignored\n",
    "        \n",
    "        # 디코딩된 텍스트 출력\n",
    "        decoded_text = tokenizer.decode(input_ids)\n",
    "        print(f\"Decoded Text: {decoded_text}\")\n",
    "        \n",
    "        # response_template 이후의 응답 부분 추출\n",
    "        if response_template in decoded_text:\n",
    "            # 템플릿 이후의 텍스트 추출\n",
    "            start_idx = decoded_text.index(response_template) + len(response_template)\n",
    "            response_text = decoded_text[start_idx:].strip()\n",
    "            \n",
    "            # 응답 끝에 있는 </s> 제거\n",
    "            if \"</s>\" in response_text:\n",
    "                response_text = response_text.split(\"</s>\")[0].strip()\n",
    "            \n",
    "            print(f\"Extracted Response: {response_text}\")\n",
    "            \n",
    "            # 라벨로 설정할 부분을 다시 토큰화\n",
    "            response_tokens = tokenizer.encode(response_text, add_special_tokens=False)\n",
    "            \n",
    "            # input_ids 내에서 해당하는 위치에 라벨 설정 (패딩 제외)\n",
    "            start_token_idx = len(tokenizer.encode(decoded_text[:start_idx], add_special_tokens=False))\n",
    "            \n",
    "            for i in range(start_token_idx, start_token_idx + len(response_tokens)):\n",
    "                labels[i] = input_ids[i]\n",
    "        \n",
    "            assert len(input_ids) == len(labels), \\\n",
    "                f\"Input IDs and labels length mismatch: {len(feature['input_ids'])} vs {len(feature['labels'])}\"\n",
    "        \n",
    "        feature['labels'] = labels \n",
    "        print(\"input_ids의 길이:\", len(input_ids))\n",
    "        print(\"labels의 길이:\", len(labels))\n",
    "        print(\"input_ids의 값:\", input_ids)\n",
    "        print(\"labels의 값:\", labels)\n",
    "    \n",
    "    return tokenizer.pad(features, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 logits 를 조정하여 정답 토큰 부분만 출력하도록 설정\n",
    "def preprocess_logits_for_metrics(logits, labels): \n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric 로드\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 정답 토큰 매핑\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# <end_of_turn> 대신 </s>로 대체하여 정답만 남기고 나머지 제거\n",
    "def extract_answer_from_label(label):\n",
    "    return label.split()[-1]\n",
    "\n",
    "# metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result \n",
    "    \n",
    "    print(\"라벨 길이: \", len(labels))\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label for label in labels if label.strip() != '']\n",
    "    labels = [label.replace(\"정답: [/INST]\\n\", \"\").strip() for label in labels]\n",
    "    labels = list(map(lambda x: x.split(\"</s>\")[0].strip(), labels)) \n",
    "    labels = list(map(extract_answer_from_label, labels)) \n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "    \n",
    "    print(\"디코딩 후 레이블: \", labels)\n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그로 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad token 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4\"\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # 예: 초기 레이어를 동결\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# # GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA를 사용할 수 없습니다. CPU로 모델을 학습합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# custom_data_collator를 tokenizer와 response_template을 고정한 상태로 생성\n",
    "fixed_data_collator = partial(custom_data_collator, tokenizer=tokenizer, response_template=\"[/INST]\")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length=4096,\n",
    "    output_dir=\"outputs_gemma\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.2,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=fixed_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True 출력 확인\n",
    "print(torch.cuda.get_device_name(0))  # Tesla V100 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from peft import PeftModel\n",
    "\n",
    "# TODO 학습된 Checkpoint 경로 입력\n",
    "checkpoint_path = \"../../data/KULLM_v2/checkpoint-1484\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    checkpoint_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data 경로 입력\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <보기>가 있을 때\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)  \n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "infer_results = []\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).half()\n",
    "\n",
    "# 모델의 forward 함수를 체크포인트로 감싸기\n",
    "def forward_with_checkpoint(input):\n",
    "    return checkpoint(model, input)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 배치 크기 설정 (메모리 사용량에 따라 조정)\n",
    "batch_size = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        \n",
    "        for data in batch:\n",
    "            _id = data[\"id\"]\n",
    "            messages = data[\"messages\"]\n",
    "            len_choices = data[\"len_choices\"]\n",
    "            \n",
    "            try:\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # CPU에서 처리\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                outputs = forward_with_checkpoint(inputs)\n",
    "                logits = outputs.logits[:, -1].flatten()\n",
    "\n",
    "                target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "                probs = torch.nn.functional.softmax(\n",
    "                    torch.tensor(target_logit_list, dtype=torch.float32),\n",
    "                    dim=-1\n",
    "                ).numpy()\n",
    "\n",
    "                predict_value = pred_choices_map[np.argmax(probs)]\n",
    "                infer_results.append({\"id\": _id, \"answer\": predict_value})\n",
    "                \n",
    "                print(f\"Processed sample {_id}: predicted {predict_value}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # 메모리 정리\n",
    "                del inputs\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"Processed {len(infer_results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
