{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, EarlyStoppingCallback, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-1380</td>\n",
       "      <td>부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스...</td>\n",
       "      <td>부산정보산업진흥원이 지원하는 2020~2021년 지역SW서비스사업화 지원사업의 주관...</td>\n",
       "      <td>[부산광역시, 중소벤처기업부, 과학기술정보통신부, 산업통상자원부, 교육부]</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1383</td>\n",
       "      <td>국무총리실은 지난 19일 오후 7시에 인사 발표 자료를 배포했다. 총리실 산하 조세...</td>\n",
       "      <td>조세심판원 상임심판관으로 임명된 J과장은 어떤 부서의 과장직을 맡고 있었던가?</td>\n",
       "      <td>[국세청, 행정자치부, 지방세분석과, 재정경제부, 세무서]</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-1384</td>\n",
       "      <td>토즈 스터디센터가 ‘겨울, 실력을 만들다’ 캠페인을 진행한다. 이 캠페인은 느슨해지...</td>\n",
       "      <td>토즈 스터디센터의 ‘겨울, 실력을 만들다’ 캠페인에서 제공하는 혜택 중 하나는 무엇인가?</td>\n",
       "      <td>[3개월 무료 이용권, 고급독서대와 스터디플래너, 장학금 100만원, 스터디센터 1...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-1386</td>\n",
       "      <td>삼성동 그랜드 인터컨티넨탈 서울 파르나스에서는 맘 놓고 외식하기도 어려워진 연말과 ...</td>\n",
       "      <td>‘그랜드 키친 홈다이닝 투고’의 판매 기간은 언제부터 언제까지인가?</td>\n",
       "      <td>[1월 1일부터 3월 1일까지, 12월 1일부터 1월 31일까지, 12월 21일부터...</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-1388</td>\n",
       "      <td>유례없는 주택시장 호황해 환호했던 2006년 봄. 뉴욕 맨해튼의 헤지펀드 매니저 존...</td>\n",
       "      <td>콘트래리언의 정의는 무엇인가?</td>\n",
       "      <td>[모두가 동의하는 방향으로 행동하는 사람, 남들과는 반대 방향으로 도전하는 사람, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                          paragraph  \\\n",
       "0  generation-for-nlp-1380  부산시와 (재)부산정보산업진흥원(원장 이인숙)이 ‘2020~2021년 지역SW서비스...   \n",
       "1  generation-for-nlp-1383  국무총리실은 지난 19일 오후 7시에 인사 발표 자료를 배포했다. 총리실 산하 조세...   \n",
       "2  generation-for-nlp-1384  토즈 스터디센터가 ‘겨울, 실력을 만들다’ 캠페인을 진행한다. 이 캠페인은 느슨해지...   \n",
       "3  generation-for-nlp-1386  삼성동 그랜드 인터컨티넨탈 서울 파르나스에서는 맘 놓고 외식하기도 어려워진 연말과 ...   \n",
       "4  generation-for-nlp-1388  유례없는 주택시장 호황해 환호했던 2006년 봄. 뉴욕 맨해튼의 헤지펀드 매니저 존...   \n",
       "\n",
       "                                            question  \\\n",
       "0  부산정보산업진흥원이 지원하는 2020~2021년 지역SW서비스사업화 지원사업의 주관...   \n",
       "1        조세심판원 상임심판관으로 임명된 J과장은 어떤 부서의 과장직을 맡고 있었던가?   \n",
       "2  토즈 스터디센터의 ‘겨울, 실력을 만들다’ 캠페인에서 제공하는 혜택 중 하나는 무엇인가?   \n",
       "3              ‘그랜드 키친 홈다이닝 투고’의 판매 기간은 언제부터 언제까지인가?   \n",
       "4                                   콘트래리언의 정의는 무엇인가?   \n",
       "\n",
       "                                             choices  answer question_plus  \n",
       "0          [부산광역시, 중소벤처기업부, 과학기술정보통신부, 산업통상자원부, 교육부]       3          None  \n",
       "1                   [국세청, 행정자치부, 지방세분석과, 재정경제부, 세무서]       3          None  \n",
       "2  [3개월 무료 이용권, 고급독서대와 스터디플래너, 장학금 100만원, 스터디센터 1...       5          None  \n",
       "3  [1월 1일부터 3월 1일까지, 12월 1일부터 1월 31일까지, 12월 21일부터...       5          None  \n",
       "4  [모두가 동의하는 방향으로 행동하는 사람, 남들과는 반대 방향으로 도전하는 사람, ...       2          None  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "id                  0\n",
      "paragraph           0\n",
      "question            0\n",
      "choices             0\n",
      "answer              0\n",
      "question_plus    2030\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2030 entries, 0 to 2029\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2030 non-null   object\n",
      " 1   paragraph      2030 non-null   object\n",
      " 2   question       2030 non-null   object\n",
      " 3   choices        2030 non-null   object\n",
      " 4   answer         2030 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 95.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF\n",
    "\n",
    "- TF-IDF 참고 링크: https://ko.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_question'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_question\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'full_question'"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_question'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTF-IDF Features:\")\n",
    "display(tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `assignment_2_persona` has been saved to /data/ephemeral/home/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `assignment_2_persona`\n"
     ]
    }
   ],
   "source": [
    "# 본인의 Huggingface auth token 입력\n",
    "## Jupyter lab에서 로그인 하는 textbox가 나오지 않을 경우, terminal에서 로그인 하실 수 있습니다.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델과 토크나이저를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2697f528387d45ca9fd2060810519b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185d315798894c5d89c1c2e52a7f3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b980fa28cc4a37aa998a3d28ad2322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"nlpai-lab/KULLM3\", \n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     quantization_config=bnb_config,\n",
    "#     trust_remote_code=True, \n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"offload\",\n",
    "# )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"upstage/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\n",
      "' + message['content']+'\n",
      "\n",
      "'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\n",
      "' + message['content']+'\n",
      "\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\n",
      "'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\n",
      "' }}{% endif %}{% endfor %}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 답을 말해주세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-425',\n",
       " 'messages': [{'role': 'system', 'content': '지문을 읽고 답을 말해주세요.'},\n",
       "  {'role': 'user',\n",
       "   'content': '지문:\\n상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服) 절차에 대하여 논한 것이 신과는 큰 차이가 있었습니다 . 장자를 위하여 3년을 입는 까닭은 위로 ‘정체(正體)’가 되기 때문이고 또 전 중(傳重: 조상의 제사나 가문의 법통을 전함)하기 때문입니다 . …(중략) … 무엇보다 중요한 것은 할아버지와 아버지의 뒤를 이은 ‘정체’이지, 꼭 첫째이기 때문에 참 최 3년 복을 입는 것은 아닙니다 .”라고 하였다 .－현종실록 －ㄱ.기 사환국으로 정권을 장악하였다 .ㄴ.인 조반정을 주도 하여 집권세력이 되었다 .ㄷ.정조 시기에 탕평 정치의 한 축을 이루었다 .ㄹ.이 이와 성혼의 문인을 중심으로 형성되었다.\\n\\n질문:\\n상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?\\n\\n선택지:\\n1 - ㄱ, ㄴ\\n2 - ㄱ, ㄷ\\n3 - ㄴ, ㄹ\\n4 - ㄷ, ㄹ\\n\\n1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\\n정답:'},\n",
       "  {'role': 'assistant', 'content': '2'}],\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95fe54e25ca47d89c6607a27d7f1672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    ) \n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[0;32m----> 2\u001b[0m bm25_corpus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tokenized_dataset]\n\u001b[1;32m      3\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(bm25_corpus)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# BM25를 이용해 길이를 1024 이하로 줄이는 함수\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[0;32m----> 2\u001b[0m bm25_corpus \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tokenized_dataset]\n\u001b[1;32m      3\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(bm25_corpus)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# BM25를 이용해 길이를 1024 이하로 줄이는 함수\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "bm25_corpus = [list(map(str, x[\"input_ids\"])) for x in tokenized_dataset]\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "\n",
    "# BM25를 이용해 길이를 1024 이하로 줄이는 함수\n",
    "def truncate_with_bm25(input_ids, bm25, max_length=1024):\n",
    "    if len(input_ids) <= max_length:\n",
    "        return input_ids  # 이미 길이가 1024 이하이면 그대로 반환\n",
    "    \n",
    "    # BM25 점수 계산\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids) \n",
    "    print(\"token 길이: \",  len(tokens))\n",
    "    scores = [bm25.get_scores([token])[0] for token in tokens]\n",
    "    print(\"scores 길이:\", len(scores))\n",
    "\n",
    "    # BM25 점수를 기준으로 상위 max_length개 선택\n",
    "    sorted_indices = np.argsort(scores)[::-1]  # 점수가 높은 순으로 정렬\n",
    "    truncated_indices = sorted_indices[:max_length]  # 상위 max_length개 선택\n",
    "    truncated_indices = sorted(truncated_indices)  # 원래 순서 유지\n",
    "\n",
    "    return [input_ids[i] for i in truncated_indices] \n",
    "  \n",
    "# Dataset에 대해 BM25 필터링 적용\n",
    "def filter_dataset_with_bm25(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    truncated_input_ids = truncate_with_bm25(input_ids, bm25, max_length=1024)\n",
    "    example[\"input_ids\"] = truncated_input_ids\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 1421\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 609\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21c068d36c74bc4b31506ccdcc7ce4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1624 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd8cf5b5704bfbbc0ddfc4cc8fb0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/406 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'train_test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터 분리\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# vram memory 제약으로 인해 인풋 데이터의 길이가 1024 초과인 데이터는 제외하였습니다. *힌트: 1024보다 길이가 더 긴 데이터를 포함하면 더 높은 점수를 달성할 수 있을 것 같습니다! \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# filtered_tokenized_dataset = tokenized_dataset.map(filter_dataset_with_bm25) \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# final_tokenized_dataset = filter_dataset_with_bm25(filtered_tokenized_dataset)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "# vram memory 제약으로 인해 인풋 데이터의 길이가 1024 초과인 데이터는 제외하였습니다. *힌트: 1024보다 길이가 더 긴 데이터를 포함하면 더 높은 점수를 달성할 수 있을 것 같습니다! \n",
    "# filtered_tokenized_dataset = tokenized_dataset.map(filter_dataset_with_bm25) \n",
    "# final_tokenized_dataset = filter_dataset_with_bm25(filtered_tokenized_dataset)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 1024)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# eos 토큰 추가하기 \n",
    "# def add_eos_token(example):\n",
    "#     example['input_ids'].append(tokenizer.eos_token_id)\n",
    "#     example['attention_mask'].append(1)\n",
    "#     return example\n",
    "\n",
    "# train_dataset = train_dataset.map(add_eos_token)\n",
    "# eval_dataset = eval_dataset.map(add_eos_token)\n",
    "\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n",
      "<s><s> [INST] <<SYS>>\n",
      "지문을 읽고 답을 말해주세요.\n",
      "<</SYS>>\n",
      "\n",
      "지문:\n",
      "\"예루살렘과 콘스탄티노플 시 경계로부터 끔찍한 이야기가 흘러나와 우리의 귀까지 들어왔습니다. 페르시아 왕국의 한 종족, 저주 받은 종족, 신으로부터 완전히 소외된 종족이 그리스도교의 땅을 침략해 칼과 약탈, 방화로 황폐화시켰다는 것입니다. 이들은 포로 중의 일부를 자국으로 끌고갔으며 어떤 이들은 잔혹한 고문으로 죽였습니다. 신의 교회를 완전히 부숴버리거나 교회를 이들의 의식에 사용해 버렸습니다… 이제 그리스 왕국은 이제 이들에 의해 쪼개졌고 2개월의 행군으로도 건널 수 없을 만큼 방대한 영토를 빼앗겼습니다. 그렇다면 이러한 만행에 대한 복수를 하고 빼앗긴 영토를 수복하는 일을 당신이 아니면 누가 하겠습니까? 다른 나라들보다 뛰어난 당신에게 주께서는 저항하는 자들의 머릿가죽을 발 아래에 두도록 무기와, 위대한 용기, 신체 능력과 강력한 힘에 놀라운 영광을 담아 내려주셨습니다. 선조들이 행한 일들이 당신들을 감동시키고, 당신의 정신을 남자다운 업적으로 이끌게 하십시오. 이교도들의 왕국을 파괴하고 성스러운 교회의 땅을 확장한 샤를마뉴 대제와 그 아들 루이 그리고 다른 왕들의 영광과 위대함을 재연하십시오. 더러운 국가들이 차지하고 있는 우리 구세주의 거룩한 무덤과, 특히 이들의 더러움으로 무자비하게 오염된 성소들에 분노하십시오. 오, 가장 용맹한 군인들이자 무적의 선조들의 후손들이여, 타락하지 말 것이며 선조들의 용맹을 떠올리십시오. 예루살렘은 세상의 배꼽입니다. 이 땅은 마치 기쁨이 흘러넘치는 낙원처럼 다른 어떤 곳보다 비옥합니다. 인류의 구세주께서는 자신의 출현으로 영광을 드러내셨고, 거주로 아름답게 하셨으며, 고난으로 거룩하게 하셨으며, 죽음으로 구원하셨고, 장례로 영광을 나타내셨습니다. 따라서 예루살렘은 세계의 중심에 위치하고 있었지만 지금은 하느님의 원수들에게 붙잡혀 있고, 하느님을 모르는 자들과 이교도들의 숭배에 지배당하고 있습니다. 이에 예루살렘을 해방을 원하며 끝없이 도움을 바라고 있습니다. 예루살렘은 특히 당신에게 도움을 청합니다. 앞서 말한 바와 같이, 주께서는 다른 모든 나라들이 아닌 당신에게 무력으로 더 큰 영광을 내리셨기 때문입니다. 따라서 천국의 썩지 아니할 영광을 확신하면서 여러분의 죄 사함을 위한 이 여행을 떠나십시오.\" 교황 우르바노 2세, 클레르몽 공의회에서 한 연설, 로버트 수도사가 기록, 1095년\n",
      "\n",
      "질문:\n",
      "다음 중 위 연설의 결과로 발생한 사건은?\n",
      "\n",
      "선택지:\n",
      "1 - 레반트 수복을 위한 기독교 운동의 실패\n",
      "2 - 지중해 무역로의 쇠퇴\n",
      "3 - 라틴계 그리스도교인들의 콘스탄티노플 정복\n",
      "4 - 비잔틴 군에 의한 셀주크 투르크의 대패\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "정답: [/INST] 3 </s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 2048\n",
      "min token length: 171\n",
      "avg token length: 1033.2832512315272\n"
     ]
    }
   ],
   "source": [
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \"당신은 고려대학교 NLP&AI 연구실에서 만든 AI 챗봇입니다. 당신의 이름은 'KULLM'으로, 한국어로는 '구름'을 뜻합니다. 당신은 비도덕적이거나, 성적이거나, 불법적이거나 또는 사회 통념적으로 허용되지 않는 발언은 하지 않습니다. 사용자와 즐겁게 대화하며, 사용자의 응답에 가능한 정확하고 친절하게 응답함으로써 최대한 도와주려고 노력합니다. 질문이 이상하다면, 어떤 부분이 이상한지 설명합니다. 거짓 정보를 발언하지 않도록 주의합니다.\" %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]'}}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion 부분만 학습하기 위한 data collator 설정\n",
    "\n",
    "- 텍스트 중 response_template 까지는 ignore_index 로 loss 계산에서 제외\n",
    "- 텍스트 중 response_template 이후는 학습에 포함 (정답 + eos 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(features, tokenizer, response_template):\n",
    "    for feature in features:\n",
    "        print(f\"Input length: {len(feature['input_ids'])}\")\n",
    "        input_ids = feature['input_ids']\n",
    "        labels = [-100] * len(input_ids)  # Initialize all labels as ignored\n",
    "        \n",
    "        # 디코딩된 텍스트 출력\n",
    "        decoded_text = tokenizer.decode(input_ids)\n",
    "        print(f\"Decoded Text: {decoded_text}\")\n",
    "        \n",
    "        # response_template 이후의 응답 부분 추출\n",
    "        if response_template in decoded_text:\n",
    "            # 템플릿 이후의 텍스트 추출\n",
    "            start_idx = decoded_text.index(response_template) + len(response_template)\n",
    "            response_text = decoded_text[start_idx:].strip()\n",
    "            \n",
    "            # 응답 끝에 있는 </s> 제거\n",
    "            if \"</s>\" in response_text:\n",
    "                response_text = response_text.split(\"</s>\")[0].strip()\n",
    "            \n",
    "            print(f\"Extracted Response: {response_text}\")\n",
    "            \n",
    "            # 라벨로 설정할 부분을 다시 토큰화\n",
    "            response_tokens = tokenizer.encode(response_text, add_special_tokens=False)\n",
    "            \n",
    "            # input_ids 내에서 해당하는 위치에 라벨 설정 (패딩 제외)\n",
    "            start_token_idx = len(tokenizer.encode(decoded_text[:start_idx], add_special_tokens=False))\n",
    "            \n",
    "            for i in range(start_token_idx, start_token_idx + len(response_tokens)):\n",
    "                labels[i] = input_ids[i]\n",
    "        \n",
    "            assert len(input_ids) == len(labels), \\\n",
    "                f\"Input IDs and labels length mismatch: {len(feature['input_ids'])} vs {len(feature['labels'])}\"\n",
    "        \n",
    "        feature['labels'] = labels \n",
    "        print(\"input_ids의 길이:\", len(input_ids))\n",
    "        print(\"labels의 길이:\", len(labels))\n",
    "        print(\"input_ids의 값:\", input_ids)\n",
    "        print(\"labels의 값:\", labels)\n",
    "    \n",
    "    return tokenizer.pad(features, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 logits 를 조정하여 정답 토큰 부분만 출력하도록 설정\n",
    "def preprocess_logits_for_metrics(logits, labels): \n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric 로드\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 정답 토큰 매핑\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# <end_of_turn> 대신 </s>로 대체하여 정답만 남기고 나머지 제거\n",
    "def extract_answer_from_label(label):\n",
    "    return label.split()[-1]\n",
    "\n",
    "# metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result \n",
    "    \n",
    "    print(\"라벨 길이: \", len(labels))\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label for label in labels if label.strip() != '']\n",
    "    labels = [label.replace(\"정답: [/INST]\\n\", \"\").strip() for label in labels]\n",
    "    labels = list(map(lambda x: x.split(\"</s>\")[0].strip(), labels)) \n",
    "    labels = list(map(extract_answer_from_label, labels)) \n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "    \n",
    "    print(\"디코딩 후 레이블: \", labels)\n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그로 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad token 설정\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# # GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA를 사용할 수 없습니다. CPU로 모델을 학습합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# custom_data_collator를 tokenizer와 response_template을 고정한 상태로 생성\n",
    "fixed_data_collator = partial(custom_data_collator, tokenizer=tokenizer, response_template=\"[/INST]\")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length=4096,\n",
    "    output_dir=\"outputs_gemma\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.2,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=fixed_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True 출력 확인\n",
    "print(torch.cuda.get_device_name(0))  # Tesla V100 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 966\n",
      "Decoded Text: <s><s> [INST] <<SYS>>\n",
      "지문을 읽고 답을 말해주세요.\n",
      "<</SYS>>\n",
      "\n",
      "지문:\n",
      "◇조선을 구한 신목, 소나무=소나무를 통해 조선의 역사를 들여다보는 책. 소나무가 조선시대의 군사, 정치, 경제, 문화 등 다양한 사회현상을 이해하는 데 매우 중요한 키워드라며 백성들의 일상생활뿐 아니라 전함을 지을 때 등 안보에 있어서도 큰 역할을 했다고 설명한다. 저자는 조선 왕실이 군사 분야에서 소나무를 어떻게 인식하고 활용했는지 조선왕조실록을 통해 꼼꼼히 확인해 나간다. (강판권 지음, 문학동네, 328쪽, 1만4000원)◇금융공학=수학, 통계학, 재무관리를 연결해 융복합적 시각으로 금융공학을 소개한다. 개념적인 설명을 통해 이론을 이해하도록 구성했고, 실무자들에게도 도움이 되도록 복합금융상품과 파생상품에 관한 다양한 형태의 실제 이슈를 다뤘다. 선물, 옵션, 스와프 등 기본적인 파생상품에 대한 내용과 주가 및 이자율의 수리적 모델링, 주가연계증권(ELS), 이자율 파생상품 및 구조화 채권 등을 소개한다.(전인태 지음, 북스힐, 262쪽, 1만5000원)◇스피치로 승부하라=기업에서는 일상화된 커뮤니케이션 방식으로 자리 잡으며 비즈니스의 핵심 역량이 된 프레젠테이션을 비롯해 연설, 강의, 면접 능력을 발전시킬 수 있는 방법을 소개한다. △청중이 무엇을 원하는지 △어떤 내용으로 청중의 마음을 움직일 수 있는지 △큐 카드와 슬라이드를 어떻게 작성하고 △커튼콜을 부르는 스피치는 어떤 것인지 등을 자세히 설명한다. (백미숙 지음, 교보문고, 272쪽, 1만5000원)\n",
      "\n",
      "질문:\n",
      "소나무가 조선시대에 중요한 역할을 한 분야가 아닌 것은 무엇인가?\n",
      "\n",
      "선택지:\n",
      "1 - 군사\n",
      "2 - 정치\n",
      "3 - 의료\n",
      "4 - 경제\n",
      "5 - 문화\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "정답: [/INST] 3 </s>\n",
      "Extracted Response: 3\n",
      "input_ids의 길이: 966\n",
      "labels의 길이: 966\n",
      "input_ids의 값: [1, 1, 733, 16289, 28793, 2087, 18741, 4060, 13, 29161, 29710, 29189, 28705, 31815, 29511, 28705, 31647, 29189, 28705, 31253, 29426, 29557, 29721, 29517, 28723, 13, 28789, 700, 18741, 4060, 13, 13, 29161, 29710, 28747, 13, 229, 154, 138, 29841, 29900, 29189, 28705, 29779, 29282, 28705, 30126, 30222, 28725, 28705, 29566, 29695, 30449, 28746, 29566, 29695, 30449, 29200, 28705, 30700, 29426, 28705, 29841, 29900, 29187, 28705, 30603, 29315, 29200, 28705, 29939, 29827, 29043, 29477, 29175, 28705, 239, 180, 136, 28723, 28705, 29566, 29695, 30449, 29135, 28705, 29841, 29900, 29236, 29634, 29187, 28705, 31672, 29315, 28725, 28705, 29233, 30159, 28725, 28705, 29653, 29526, 28725, 28705, 29710, 29731, 28705, 30390, 28705, 29043, 31067, 29282, 28705, 29315, 29884, 30205, 29578, 29189, 28705, 29015, 29426, 29136, 29175, 28705, 29969, 28705, 30556, 29687, 28705, 30027, 29517, 29282, 28705, 30649, 31126, 29470, 29700, 31718, 28705, 31250, 29465, 29939, 29187, 28705, 29415, 29578, 29693, 31273, 238, 194, 147, 28705, 29536, 29194, 29700, 28705, 29600, 29938, 29189, 28705, 29161, 29189, 28705, 30178, 28705, 30390, 28705, 30325, 29477, 29148, 28705, 29604, 29433, 29305, 29599, 28705, 31694, 28705, 30603, 29723, 29189, 28705, 30676, 29043, 29511, 28705, 30058, 29955, 29282, 29043, 28723, 28705, 29998, 29294, 29175, 28705, 29841, 29900, 28705, 239, 156, 152, 29991, 29015, 28705, 31672, 29315, 28705, 30217, 30263, 29148, 29305, 28705, 29566, 29695, 30449, 29200, 28705, 29433, 238, 153, 190, 29833, 28705, 29324, 30355, 29136, 29511, 28705, 31273, 29424, 30676, 29175, 29161, 28705, 29841, 29900, 239, 156, 152, 29841, 29991, 29832, 29189, 28705, 30700, 29426, 28705, 237, 191, 191, 237, 191, 191, 31909, 28705, 30270, 29324, 29426, 28705, 29695, 30112, 29043, 28723, 325, 30967, 31334, 31579, 28705, 29161, 29881, 28725, 28705, 29710, 30711, 29714, 30959, 28725, 28705, 28770, 28750, 28783, 31117, 28725, 28705, 28740, 29757, 28781, 28734, 28734, 28734, 29816, 28731, 229, 154, 138, 30331, 239, 159, 184, 30010, 30711, 28746, 29151, 30711, 28725, 28705, 30700, 30106, 30711, 28725, 28705, 30014, 30449, 30224, 29288, 29200, 28705, 30107, 30075, 29426, 28705, 239, 159, 184, 30357, 29770, 30151, 28705, 29236, 30750, 29583, 29143, 28705, 30331, 239, 159, 184, 30010, 30711, 29189, 28705, 29566, 29893, 29282, 29043, 28723, 28705, 29893, 238, 136, 147, 30151, 29324, 28705, 30058, 29955, 29189, 28705, 30700, 29426, 28705, 29015, 238, 164, 163, 29189, 28705, 29015, 29426, 29136, 29599, 29832, 28705, 29779, 29465, 30676, 29511, 28725, 28705, 29991, 30449, 29294, 29939, 29148, 29833, 29599, 28705, 29599, 239, 158, 131, 29015, 28705, 29848, 29599, 29832, 28705, 30357, 29770, 30331, 239, 159, 184, 29578, 30650, 29844, 28705, 29806, 29693, 29578, 30650, 29148, 28705, 30224, 29282, 28705, 29043, 31067, 29282, 28705, 30498, 30533, 29187, 28705, 29991, 29526, 28705, 29015, 239, 141, 139, 29200, 28705, 29043, 238, 167, 155, 29043, 28723, 28705, 29900, 30895, 28725, 28705, 31629, 30457, 28725, 28705, 29304, 30275, 30212, 28705, 30390, 28705, 29164, 30535, 30151, 29324, 28705, 29806, 29693, 29578, 30650, 29148, 28705, 29634, 29282, 28705, 29911, 29424, 29844, 28705, 29557, 29135, 28705, 31262, 28705, 29015, 29294, 239, 159, 171, 29187, 28705, 29151, 29288, 30151, 28705, 29820, 238, 144, 187, 31689, 28725, 28705, 29557, 29135, 30107, 30106, 30713, 31579, 28732, 2980, 28735, 557, 28705, 29015, 29294, 239, 159, 171, 28705, 29806, 29693, 29578, 30650, 28705, 31262, 28705, 29779, 29841, 29731, 28705, 31556, 31579, 28705, 30390, 29189, 28705, 29566, 29893, 29282, 29043, 21057, 29600, 29324, 30533, 28705, 29161, 29881, 28725, 28705, 238, 185, 132, 29304, 240, 161, 147, 28725, 28705, 28750, 28784, 28750, 31117, 28725, 28705, 28740, 29757, 28782, 28734, 28734, 28734, 29816, 28731, 229, 154, 138, 29304, 31306, 30159, 29143, 28705, 31673, 29775, 29136, 29700, 28746, 29164, 30279, 29148, 29305, 29175, 28705, 29415, 29578, 29731, 29897, 28705, 31582, 238, 177, 167, 29194, 239, 191, 131, 29015, 30457, 28705, 30240, 30355, 29583, 29143, 28705, 29294, 29288, 28705, 239, 161, 164, 29583, 31718, 28705, 29859, 239, 169, 139, 29194, 29304, 29187, 28705, 240, 152, 184, 31987, 28705, 30603, 31314, 29015, 28705, 29897, 28705, 30212, 30514, 239, 163, 163, 30566, 29015, 30457, 29189, 28705, 29859, 238, 164, 178, 29426, 28705, 30107, 30058, 28725, 28705, 30967, 29187, 28725, 28705, 29565, 31022, 28705, 30364, 29626, 29189, 28705, 30482, 29600, 29236, 240, 133, 175, 28705, 29151, 28705, 29604, 29175, 28705, 30240, 30979, 29189, 28705, 29566, 29893, 29282, 29043, 28723, 28705, 229, 153, 182, 30594, 30027, 29015, 28705, 30449, 239, 154, 138, 29189, 28705, 29816, 29136, 29175, 29161, 28705, 229, 153, 182, 29433, 238, 153, 167, 28705, 29911, 29424, 29583, 29143, 28705, 30594, 30027, 29187, 28705, 30103, 29881, 29189, 28705, 239, 158, 131, 30892, 29415, 28705, 29151, 28705, 29604, 29175, 29161, 28705, 229, 153, 182, 240, 132, 147, 28705, 30693, 29470, 30275, 28705, 239, 141, 175, 29700, 29015, 29470, 29200, 28705, 29433, 238, 153, 190, 29833, 28705, 29781, 29465, 29136, 29511, 28705, 229, 153, 182, 31582, 31285, 239, 192, 159, 29189, 28705, 29775, 31091, 29175, 28705, 29304, 31306, 30159, 29175, 28705, 29433, 238, 153, 167, 28705, 30439, 29324, 29161, 28705, 30390, 29189, 28705, 29294, 29721, 31909, 28705, 30058, 29955, 29282, 29043, 28723, 325, 31250, 29967, 239, 139, 156, 28705, 29161, 29881, 28725, 28705, 31012, 29477, 29710, 29511, 28725, 28705, 28750, 28787, 28750, 31117, 28725, 28705, 28740, 29757, 28782, 28734, 28734, 28734, 29816, 28731, 13, 13, 31959, 29710, 28747, 13, 29566, 29695, 30449, 29135, 28705, 29841, 29900, 29236, 29634, 29148, 28705, 30027, 29517, 29282, 28705, 30603, 29723, 29189, 28705, 29282, 28705, 30217, 30263, 29135, 28705, 29536, 238, 142, 143, 28705, 30439, 29538, 28705, 30449, 239, 154, 138, 29324, 29135, 28804, 13, 13, 29900, 30442, 29161, 28747, 13, 28740, 387, 28705, 31672, 29315, 13, 28750, 387, 28705, 29233, 30159, 13, 28770, 387, 28705, 29187, 30221, 13, 28781, 387, 28705, 29653, 29526, 13, 28782, 387, 28705, 29710, 29731, 13, 13, 28740, 28725, 28705, 28750, 28725, 28705, 28770, 28725, 28705, 28781, 28725, 28705, 28782, 28705, 30027, 29148, 28705, 29136, 29695, 29200, 28705, 29233, 31647, 29583, 29143, 28705, 29511, 31091, 29721, 29517, 28723, 13, 29233, 31647, 28747, 733, 28748, 16289, 28793, 28705, 28770, 28705, 2]\n",
      "labels의 값: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 28770, 28705, -100]\n",
      "Input length: 2048\n",
      "Decoded Text: <s><s> [INST] <<SYS>>\n",
      "지문을 읽고 답을 말해주세요.\n",
      "<</SYS>>\n",
      "\n",
      "지문:\n",
      "미국에서 4일(현지시간) 치러진 중간선거에서 야당인 공화당이 상·하원을 모두 장악한 것은 유권자들이 버락 오바마 대통령의 국정운영 능력을 냉혹하게 평가한 결과다. 워싱턴포스트(WP)는 5일 “이번 선거에서 국민들이 오바마 대통령의 리더십에 환멸을 느끼고 있다는 점이 여실히 드러났다”고 분석했다. 임기를 2년 남겨 둔 오바마 대통령은 급격한 레임덕(권력누수)에 빠지고, 2016년 차기 대권경쟁이 본격화할 것이란 전망이다.○공화당 의회권력 독차지공화당은 하원 다수당 자리를 유지하면서 상원까지 장악했다. 노스캐롤라이나 콜로라도 아이오와 웨스트버지니아 아칸소 몬태나 사우스다코타주의 민주당 상원의석을 빼앗았다. 기존엔 45석이었으나 5일 오전 10시(현지시간) 현재 최소 52석을 확보해 전체 100석 중 절반을 넘었다. 의회권력이 완전히 공화당으로 넘어가고 2006년 조지 W 부시 대통령 때 민주당이 양원을 장악한 이래 8년 만에 ‘여소야대’ 정국이 탄생했다.‘다수당 독식’ 원칙에 따라 공화당은 하원에 이어 상원 상임위원장도 모두 차지한다. 미 의회 의사결정이 철저히 다수결 원칙으로 이뤄진다는 점을 고려하면 공화당은 오바마케어(건강보험개혁법)·온실가스규제 행정명령 등 오바마 행정부의 주요 정책을 법률로 무효화할 수도 있다. 하원 435명 전원을 뽑는 하원 선거에서도 공화당은 의석 수를 현재 233석에서 최소 242석 이상으로 늘릴 것으로 보인다. 이는 해리 트루먼 대통령 이후 64년 만에 공화당의 최다 하원 의석으로 기록될 전망이다. 공화당은 주지사 선거에서도 압승했다.○워싱턴 정치실종에 오바마 책임 물어전문가들은 공화당의 승리에 대해 “공화당 선호가 높아서가 아니라 오바마 대통령의 리더십에 대한 유권자들의 실망과 그에 따른 반사이익이 크게 작용했다”고 분석했다. 출구 여론조사 결과 투표자의 60%가 오바마 행정부에 대해 ‘불만스럽다’ 또는 ‘화가 난다’고 응답했지만, 마찬가지로 60%가 공화당 지도자들에게 불만을 표시했다. 미국인들이 오바마 대통령 재선 이후 2년 동안 법률 하나 제대로 통과시키지 못하는 정치권의 교착상태에 염증이 나 있었다는 것이다. 실제로 경제 회복과 직결되는 이민법개혁, 세제개혁, 인프라 투자 확대 등이 수년째 의회에 발목이 잡혀 있다. 월스트리트저널(WSJ)은 “미약한 경제 회복, 그리고 이슬람 수니파 극단주의 무장세력 이슬람국가(IS) 및 에볼라에 대처하는 과정에서 오바마 행정부의 신뢰가 떨어지면서 유권자들이 등을 돌렸다”고 분석했다. ○오바마 국정운영 기조 바꾸나오바마 대통령은 기로에 서게 됐다. 의회와 담을 쌓고 ‘행정명령’을 발동하는 기존 국정운영 스타일을 고수할지, 아니면 세제개혁·자유무역협정(FTA) 등 공통 관심사안에 대해 공화당과 손을 잡고 타협의 정치를 할지 선택해야 한다. 상원 다수당 대표가 될 미치 매코널 공화당 원내대표는 당선 연설에서 “우리는 서로 동의하는 이슈를 함께 풀어야 할 의무가 있다”며 “양당 시스템을 갖고 있다고 해서 영구적으로 충돌해야 한다는 건 아니다”고 말했다. 오바마와 민주당 측에 타협의 정치를 촉구한 셈이다. 오바마 대통령은 7일 양당 지도부를 백악관에 초청해 회동 할 예정이다.워싱턴 정가는 오바마 대통령이 남은 재임기간 ‘업적(legacy)’을 만들기 위해서라도 공화당과 대타협을 모색할 가능성을 높게 보고 있다. 그 첫 시험대가 이민개혁법과 법인세 개혁이 될 것이라고 WSJ는 보도했다.\n",
      "\n",
      "질문:\n",
      "이번 중간선거에서 공화당이 상원과 하원을 모두 장악한 이유는 무엇인가?\n",
      "\n",
      "선택지:\n",
      "1 - 공화당의 정책이 유권자들에게 인기를 끌었기 때문\n",
      "2 - 민주당의 후보들이 부적절했기 때문\n",
      "3 - 유권자들이 오바마 대통령의 국정운영 능력에 실망했기 때문\n",
      "4 - 오바마 대통령의 개인적\n",
      "input_ids의 길이: 2048\n",
      "labels의 길이: 2048\n",
      "input_ids의 값: [1, 1, 733, 16289, 28793, 2087, 18741, 4060, 13, 29161, 29710, 29189, 28705, 31815, 29511, 28705, 31647, 29189, 28705, 31253, 29426, 29557, 29721, 29517, 28723, 13, 28789, 700, 18741, 4060, 13, 13, 29161, 29710, 28747, 13, 29967, 30779, 29148, 29305, 28705, 28781, 29415, 28732, 30205, 29161, 29236, 30112, 28731, 28705, 30159, 30244, 30345, 28705, 30027, 30112, 29900, 30013, 29148, 29305, 28705, 30263, 30287, 29324, 28705, 30010, 29731, 30287, 29015, 28705, 29578, 28854, 29136, 29816, 29189, 28705, 29820, 30609, 28705, 29747, 239, 152, 136, 29282, 28705, 30439, 29538, 28705, 30127, 31579, 29294, 29939, 29015, 28705, 30041, 238, 160, 192, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29187, 28705, 30779, 29233, 30553, 30478, 28705, 30364, 29626, 29189, 28705, 238, 134, 140, 240, 155, 188, 29136, 29833, 28705, 31523, 29135, 29282, 28705, 30075, 29844, 29043, 28723, 28705, 31126, 239, 142, 180, 31167, 30387, 29304, 29404, 28732, 23937, 28731, 29175, 28705, 28782, 29415, 981, 29015, 29756, 28705, 29900, 30013, 29148, 29305, 28705, 30779, 31463, 29939, 29015, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29187, 28705, 29288, 30519, 31202, 29148, 28705, 30391, 238, 172, 187, 29189, 28705, 238, 141, 147, 238, 132, 191, 29511, 28705, 29604, 29043, 29175, 28705, 30589, 29015, 28705, 29827, 29991, 31909, 28705, 29470, 30244, 238, 133, 175, 29043, 28838, 29511, 28705, 30217, 31585, 30676, 29043, 28723, 28705, 30520, 29164, 29200, 28705, 28750, 31479, 28705, 31183, 237, 181, 171, 28705, 238, 148, 151, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29538, 28705, 31563, 31110, 29282, 28705, 30514, 30520, 238, 144, 152, 28732, 31579, 29626, 31596, 29151, 28731, 29148, 28705, 238, 188, 163, 29161, 29511, 28725, 28705, 28750, 28734, 28740, 28784, 31479, 28705, 30734, 29164, 28705, 29634, 31579, 29653, 239, 162, 132, 29015, 28705, 30535, 31110, 29731, 29723, 28705, 30439, 29015, 238, 161, 131, 28705, 29600, 238, 170, 160, 29015, 29043, 28723, 31269, 30010, 29731, 30287, 28705, 29187, 29884, 31579, 29626, 28705, 31930, 30734, 29161, 30010, 29731, 30287, 29538, 28705, 29136, 29816, 28705, 29043, 29151, 30287, 28705, 29294, 29288, 29200, 28705, 30127, 29161, 29136, 29565, 29305, 28705, 29578, 29816, 30578, 29161, 28705, 29747, 239, 152, 136, 30676, 29043, 28723, 28705, 30525, 29304, 239, 189, 147, 31953, 29700, 29015, 29695, 28705, 239, 192, 159, 29143, 29700, 29599, 28705, 29536, 29015, 29647, 30275, 28705, 239, 158, 171, 29304, 29404, 30041, 29161, 29194, 29536, 28705, 29536, 239, 188, 187, 29566, 28705, 238, 173, 175, 30533, 29695, 28705, 29315, 29687, 29304, 29043, 30293, 30304, 29557, 29187, 28705, 31463, 29557, 30287, 28705, 29578, 29816, 29187, 31585, 29189, 28705, 238, 188, 191, 239, 152, 154, 31877, 29043, 28723, 28705, 29164, 31025, 31821, 28705, 28781, 28782, 31585, 29015, 30591, 29583, 29695, 28705, 28782, 29415, 28705, 29647, 29600, 28705, 28740, 28734, 29236, 28732, 30205, 29161, 29236, 30112, 28731, 28705, 30205, 30014, 28705, 30310, 29566, 28705, 28782, 28750, 31585, 29189, 28705, 30270, 29477, 29426, 28705, 29600, 29823, 28705, 28740, 28734, 28734, 31585, 28705, 30027, 28705, 239, 163, 139, 30192, 29189, 28705, 238, 135, 155, 30591, 29043, 28723, 28705, 29187, 29884, 31579, 29626, 29015, 28705, 31068, 29600, 31909, 28705, 30010, 29731, 30287, 29583, 29143, 28705, 238, 135, 155, 29433, 29135, 29511, 28705, 28750, 28734, 28734, 28784, 31479, 28705, 29841, 29161, 394, 28705, 29775, 29236, 28705, 29634, 30700, 31514, 28705, 30178, 28705, 31463, 29557, 30287, 29015, 28705, 31067, 29816, 29189, 28705, 29747, 239, 152, 136, 29282, 28705, 29015, 29889, 28705, 28783, 31479, 28705, 29757, 29148, 3475, 29827, 29566, 30263, 29634, 28809, 28705, 29233, 30779, 29015, 28705, 240, 134, 135, 29693, 30676, 29043, 28723, 28894, 29043, 29151, 30287, 28705, 31930, 30355, 28809, 28705, 29816, 239, 188, 156, 29148, 28705, 31359, 29700, 28705, 30010, 29731, 30287, 29538, 28705, 29136, 29816, 29148, 28705, 29015, 29433, 28705, 29578, 29816, 28705, 29578, 30520, 29744, 29816, 29747, 29599, 28705, 29820, 30609, 28705, 30734, 29161, 29282, 29043, 28723, 28705, 29967, 28705, 29187, 29884, 28705, 29187, 29315, 30075, 29233, 29015, 28705, 239, 181, 163, 29998, 31909, 28705, 29043, 29151, 30075, 28705, 29816, 239, 188, 156, 29583, 29143, 28705, 29015, 238, 167, 135, 30345, 29043, 29175, 28705, 30589, 29189, 28705, 29511, 30710, 29136, 29565, 28705, 30010, 29731, 30287, 29538, 28705, 29647, 30140, 30103, 239, 191, 131, 29433, 28732, 30771, 30967, 29477, 240, 154, 155, 29893, 240, 155, 132, 30979, 28731, 28854, 31386, 29991, 29135, 29304, 31982, 29526, 28705, 30094, 29233, 29955, 31514, 28705, 30390, 28705, 29647, 30140, 30103, 28705, 30094, 29233, 29775, 29187, 28705, 29557, 29517, 28705, 29233, 239, 180, 136, 29189, 28705, 30979, 238, 168, 163, 29143, 28705, 30449, 31481, 29731, 29723, 28705, 29151, 29599, 28705, 29604, 29043, 28723, 28705, 29136, 29816, 28705, 28781, 28770, 28782, 29955, 28705, 29600, 29816, 29189, 28705, 238, 192, 148, 29175, 28705, 29136, 29816, 28705, 29900, 30013, 29148, 29305, 29599, 28705, 30010, 29731, 30287, 29538, 28705, 29187, 31585, 28705, 29151, 29200, 28705, 30205, 30014, 28705, 28750, 28770, 28770, 31585, 29148, 29305, 28705, 30310, 29566, 28705, 28750, 28781, 28750, 31585, 28705, 29015, 29578, 29583, 29143, 28705, 238, 141, 155, 238, 169, 183, 28705, 30439, 29583, 29143, 28705, 29477, 29324, 29043, 28723, 28705, 29015, 29175, 28705, 29426, 29288, 28705, 29404, 31602, 238, 171, 191, 28705, 29634, 30700, 31514, 28705, 29015, 30570, 28705, 28784, 28781, 31479, 28705, 29757, 29148, 28705, 30010, 29731, 30287, 29187, 28705, 30310, 29043, 28705, 29136, 29816, 28705, 29187, 31585, 29583, 29143, 28705, 29164, 29832, 31705, 28705, 29600, 238, 170, 160, 29015, 29043, 28723, 28705, 30010, 29731, 30287, 29538, 28705, 29557, 29161, 29315, 28705, 29900, 30013, 29148, 29305, 29599, 28705, 239, 152, 152, 31673, 30676, 29043, 28723, 31269, 31126, 239, 142, 180, 31167, 28705, 29233, 30159, 29991, 30490, 29148, 28705, 29647, 30140, 30103, 28705, 239, 180, 136, 30520, 28705, 30895, 29433, 29600, 29710, 29135, 29939, 29538, 28705, 30010, 29731, 30287, 29187, 28705, 31673, 29288, 29148, 28705, 29634, 29426, 981, 30010, 29731, 30287, 28705, 29900, 29730, 29135, 28705, 238, 137, 149, 29536, 29305, 29135, 28705, 29536, 29194, 29700, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29187, 28705, 29288, 30519, 31202, 29148, 28705, 29634, 29282, 28705, 30127, 31579, 29294, 29939, 29187, 28705, 29991, 238, 170, 160, 29844, 28705, 29614, 29148, 28705, 31359, 30879, 28705, 30192, 29315, 29015, 239, 160, 184, 29015, 28705, 30056, 29833, 28705, 29781, 29424, 30676, 29043, 28838, 29511, 28705, 30217, 31585, 30676, 29043, 28723, 28705, 29858, 29779, 28705, 29827, 238, 164, 163, 29841, 29315, 28705, 30075, 29844, 28705, 240, 139, 175, 30191, 29294, 29187, 28705, 28784, 28734, 28823, 29135, 28705, 29647, 30140, 30103, 28705, 30094, 29233, 29775, 29148, 28705, 29634, 29426, 3475, 30665, 29757, 29304, 238, 162, 192, 29043, 28809, 28705, 31292, 29175, 3475, 29731, 29135, 28705, 31876, 29043, 28809, 29511, 28705, 239, 160, 148, 31647, 30676, 29161, 29757, 28725, 28705, 30103, 239, 179, 175, 29135, 29161, 29143, 28705, 28784, 28734, 28823, 29135, 28705, 30010, 29731, 30287, 28705, 29161, 29599, 29294, 29939, 29148, 29833, 28705, 30665, 29757, 29189, 28705, 30191, 29236, 30676, 29043, 28723, 28705, 29967, 30779, 29324, 29939, 29015, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 28705, 30014, 29900, 28705, 29015, 30570, 28705, 28750, 31479, 28705, 29714, 30325, 28705, 30979, 238, 168, 163, 28705, 29136, 29695, 28705, 29526, 29634, 29143, 28705, 30700, 29844, 29236, 30649, 29161, 28705, 31408, 29136, 29175, 28705, 29233, 30159, 31579, 29187, 28705, 31012, 239, 179, 172, 29578, 30533, 29148, 28705, 239, 154, 191, 30713, 29015, 28705, 29695, 28705, 29604, 30591, 29043, 29175, 28705, 30439, 29015, 29043, 28723, 28705, 29991, 29526, 29143, 28705, 29653, 29526, 28705, 29884, 30357, 29844, 28705, 30892, 30075, 29848, 29175, 28705, 29015, 31463, 30979, 29893, 240, 155, 132, 28725, 28705, 29721, 29526, 29893, 240, 155, 132, 28725, 28705, 29324, 30212, 29700, 28705, 240, 139, 175, 29294, 28705, 30270, 29634, 28705, 30390, 29015, 28705, 29151, 31479, 31003, 28705, 29187, 29884, 29148, 28705, 30482, 30222, 29015, 28705, 239, 161, 164, 240, 155, 131, 28705, 29604, 29043, 28723, 28705, 30839, 29304, 29404, 29288, 29404, 29998, 238, 135, 147, 28732, 7965, 28798, 28731, 29538, 981, 29967, 31160, 29282, 28705, 29653, 29526, 28705, 29884, 30357, 28725, 28705, 29614, 29288, 29511, 28705, 29015, 239, 141, 175, 31390, 28705, 29151, 29194, 29806, 28705, 237, 186, 188, 30315, 29557, 29187, 28705, 30449, 29747, 29721, 29626, 28705, 29015, 239, 141, 175, 31390, 30779, 29135, 28732, 1851, 28731, 28705, 31262, 28705, 29148, 238, 182, 191, 29700, 29148, 28705, 29634, 30201, 29136, 29175, 28705, 29844, 29233, 29148, 29305, 28705, 29647, 30140, 30103, 28705, 30094, 29233, 29775, 29187, 28705, 30126, 238, 165, 179, 29135, 28705, 238, 153, 171, 29433, 29161, 29565, 29305, 28705, 30127, 31579, 29294, 29939, 29015, 28705, 30390, 29189, 28705, 31597, 238, 163, 187, 29043, 28838, 29511, 28705, 30217, 31585, 30676, 29043, 28723, 28705, 31269, 29647, 30140, 30103, 28705, 30779, 29233, 30553, 30478, 28705, 29164, 29841, 28705, 30140, 237, 193, 187, 29695, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29538, 28705, 29164, 29143, 29148, 28705, 29305, 29833, 28705, 238, 147, 147, 29043, 28723, 28705, 29187, 29884, 30275, 28705, 31569, 29189, 28705, 239, 143, 150, 29511, 3475, 30094, 29233, 29955, 31514, 28809, 29189, 28705, 30482, 29714, 29136, 29175, 28705, 29164, 31025, 28705, 30779, 29233, 30553, 30478, 28705, 29304, 30304, 29415, 29189, 28705, 29511, 29151, 29723, 29161, 28725, 28705, 29536, 29194, 29565, 28705, 29721, 29526, 29893, 240, 155, 132, 28854, 29294, 30127, 30449, 30603, 240, 155, 148, 29233, 28732, 5550, 28741, 28731, 28705, 30390, 28705, 30010, 30700, 28705, 30224, 31987, 29315, 30325, 29148, 28705, 29634, 29426, 28705, 30010, 29731, 30287, 29844, 28705, 239, 137, 147, 29189, 28705, 239, 161, 164, 29511, 28705, 30304, 240, 155, 148, 29187, 28705, 29233, 30159, 29200, 28705, 29723, 29161, 28705, 29900, 30442, 29426, 30263, 28705, 29282, 29043, 28723, 28705, 29578, 29816, 28705, 29043, 29151, 30287, 28705, 29634, 30191, 29135, 28705, 31705, 28705, 29967, 30159, 28705, 30556, 30293, 238, 135, 147, 28705, 30010, 29731, 30287, 28705, 29816, 29911, 29634, 30191, 29175, 28705, 30287, 29900, 28705, 30107, 30058, 29148, 29305, 981, 29687, 29288, 29175, 28705, 29305, 29143, 28705, 29714, 29187, 29136, 29175, 28705, 29015, 239, 141, 139, 29200, 28705, 29938, 237, 190, 155, 28705, 240, 149, 131, 29433, 30263, 28705, 29723, 28705, 29187, 30449, 29135, 28705, 29604, 29043, 28838, 31718, 981, 31067, 30287, 28705, 29236, 29304, 31485, 29189, 28705, 237, 179, 153, 29511, 28705, 29604, 29043, 29511, 28705, 29426, 29305, 28705, 30478, 29779, 30151, 29583, 29143, 28705, 239, 185, 172, 31597, 29426, 30263, 28705, 29282, 29043, 29175, 28705, 30771, 28705, 29536, 29194, 29043, 28838, 29511, 28705, 31253, 30676, 29043, 28723, 28705, 29647, 30140, 30103, 30275, 28705, 31463, 29557, 30287, 28705, 239, 187, 164, 29148, 28705, 30304, 240, 155, 148, 29187, 28705, 29233, 30159, 29200, 28705, 239, 183, 140, 29779, 29282, 28705, 239, 136, 139, 29015, 29043, 28723, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29538, 28705, 28787, 29415, 28705, 31067, 30287, 28705, 29161, 29599, 29775, 29200, 28705, 31250, 239, 152, 136, 30224, 29148, 28705, 30395, 30594, 29426, 28705, 29884, 29714, 28705, 29723, 28705, 30542, 29233, 29015, 29043, 28723, 31126, 239, 142, 180, 31167, 28705, 29233, 29135, 29175, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29015, 28705, 31183, 29538, 28705, 30014, 30520, 29164, 30112, 3475, 30279, 30151, 28732, 1946, 2426, 28731, 28809, 29189, 28705, 29757, 29939, 29164, 28705, 29744, 29426, 29305, 29700, 29599, 28705, 30010, 29731, 30287, 29844, 28705, 29634, 30304, 240, 155, 148, 29189, 28705, 29820, 30508, 29723, 28705, 29135, 30364, 29465, 29189, 28705, 238, 137, 149, 29833, 28705, 29477, 29511, 28705, 29604, 29043, 28723, 28705, 29614, 28705, 31745, 28705, 29236, 240, 154, 155, 29634, 29135, 28705, 29015, 31463, 29893, 240, 155, 132, 30979, 29844, 28705, 30979, 29324, 29721, 28705, 29893, 240, 155, 132, 29015, 28705, 31705, 28705, 30439, 29015, 29700, 29511, 394, 28735, 28798, 29175, 28705, 29477, 29599, 30676, 29043, 28723, 13, 13, 31959, 29710, 28747, 13, 29015, 29756, 28705, 30027, 30112, 29900, 30013, 29148, 29305, 28705, 30010, 29731, 30287, 29015, 28705, 29578, 29816, 29844, 28705, 29136, 29816, 29189, 28705, 29820, 30609, 28705, 29747, 239, 152, 136, 29282, 28705, 29015, 30127, 29175, 28705, 30449, 239, 154, 138, 29324, 29135, 28804, 13, 13, 29900, 30442, 29161, 28747, 13, 28740, 387, 28705, 30010, 29731, 30287, 29187, 28705, 29233, 239, 180, 136, 29015, 28705, 30127, 31579, 29294, 29939, 29148, 29833, 28705, 29324, 29164, 29200, 28705, 238, 132, 143, 30591, 29164, 28705, 30178, 29710, 13, 28750, 387, 28705, 31463, 29557, 30287, 29187, 28705, 30570, 29477, 29939, 29015, 28705, 29775, 30151, 239, 163, 139, 30676, 29164, 28705, 30178, 29710, 13, 28770, 387, 28705, 30127, 31579, 29294, 29939, 29015, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29187, 28705, 30779, 29233, 30553, 30478, 28705, 30364, 29626, 29148, 28705, 29991, 238, 170, 160, 30676, 29164, 28705, 30178, 29710, 13, 28781, 387, 28705, 29647, 30140, 30103, 28705, 29634, 30700, 31514, 29187, 28705, 29893, 29324, 30151]\n",
      "labels의 값: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Input length: 306\n",
      "Decoded Text: <s><s> [INST] <<SYS>>\n",
      "지문을 읽고 답을 말해주세요.\n",
      "<</SYS>>\n",
      "\n",
      "지문:\n",
      "금리     신규 주택     실업률\n",
      "\n",
      "질문:\n",
      "한 국가가 완전 고용 상태이고 중앙은행이 긴축적 통화 정책을 시행하는 경우, 다음 중 금리, 신규 주택 구입, 실업률이 다음 중 어떤 방식으로 변화할 것으로 예상할 수 있습니까?\n",
      "\n",
      "선택지:\n",
      "1 - 감소     증가     증가\n",
      "2 - 감소     감소     감소\n",
      "3 - 증가     감소     감소\n",
      "4 - 증가     감소     증가\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "정답: [/INST] 4 </s>\n",
      "Extracted Response: 4\n",
      "input_ids의 길이: 306\n",
      "labels의 길이: 306\n",
      "input_ids의 값: [1, 1, 733, 16289, 28793, 2087, 18741, 4060, 13, 29161, 29710, 29189, 28705, 31815, 29511, 28705, 31647, 29189, 28705, 31253, 29426, 29557, 29721, 29517, 28723, 13, 28789, 700, 18741, 4060, 13, 13, 29161, 29710, 28747, 13, 30331, 29288, 28705, 29000, 29000, 29000, 28705, 30126, 31982, 28705, 29557, 30442, 28705, 29000, 29000, 29000, 28705, 29991, 30279, 238, 168, 163, 13, 13, 31959, 29710, 28747, 13, 29282, 28705, 30779, 29135, 29135, 28705, 31068, 29600, 28705, 29511, 29424, 28705, 29578, 30533, 29015, 29511, 28705, 30027, 239, 152, 156, 29538, 30094, 29015, 28705, 237, 187, 183, 239, 185, 152, 30151, 28705, 30700, 29731, 28705, 29233, 239, 180, 136, 29189, 28705, 29236, 30094, 29136, 29175, 28705, 29653, 29687, 28725, 28705, 29043, 29881, 28705, 30027, 28705, 30331, 29288, 28725, 28705, 30126, 31982, 28705, 29557, 30442, 28705, 29779, 29429, 28725, 28705, 29991, 30279, 238, 168, 163, 29015, 28705, 29043, 29881, 28705, 30027, 28705, 29433, 238, 153, 167, 28705, 30240, 30355, 29583, 29143, 28705, 29901, 29731, 29723, 28705, 30439, 29583, 29143, 28705, 30542, 29578, 29723, 28705, 29151, 28705, 29604, 29570, 29194, 30578, 28804, 13, 13, 29900, 30442, 29161, 28747, 13, 28740, 387, 28705, 31402, 29566, 28705, 29000, 29000, 29000, 28705, 30713, 29135, 28705, 29000, 29000, 29000, 28705, 30713, 29135, 13, 28750, 387, 28705, 31402, 29566, 28705, 29000, 29000, 29000, 28705, 31402, 29566, 28705, 29000, 29000, 29000, 28705, 31402, 29566, 13, 28770, 387, 28705, 30713, 29135, 28705, 29000, 29000, 29000, 28705, 31402, 29566, 28705, 29000, 29000, 29000, 28705, 31402, 29566, 13, 28781, 387, 28705, 30713, 29135, 28705, 29000, 29000, 29000, 28705, 31402, 29566, 28705, 29000, 29000, 29000, 28705, 30713, 29135, 13, 13, 28740, 28725, 28705, 28750, 28725, 28705, 28770, 28725, 28705, 28781, 28725, 28705, 28782, 28705, 30027, 29148, 28705, 29136, 29695, 29200, 28705, 29233, 31647, 29583, 29143, 28705, 29511, 31091, 29721, 29517, 28723, 13, 29233, 31647, 28747, 733, 28748, 16289, 28793, 28705, 28781, 28705, 2]\n",
      "labels의 값: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 28781, 28705, -100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 390.38 MiB is free. Process 1307010 has 31.36 GiB memory in use. Of the allocated memory 30.44 GiB is allocated by PyTorch, and 553.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:373\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m causal_mask\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    374\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    375\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 390.38 MiB is free. Process 1307010 has 31.36 GiB memory in use. Of the allocated memory 30.44 GiB is allocated by PyTorch, and 553.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1736ed60b740d8ba21ab5bb4b22300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from peft import PeftModel\n",
    "\n",
    "# TODO 학습된 Checkpoint 경로 입력\n",
    "checkpoint_path = \"../../data/KULLM_v2/checkpoint-1484\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    checkpoint_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpai-lab/KULLM3\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data 경로 입력\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <보기>가 있을 때\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]<timed exec>:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 869/869 [38:21<00:00,  2.65s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 41s, sys: 13min 38s, total: 38min 20s\n",
      "Wall time: 38min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)  \n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "infer_results = []\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).half()\n",
    "\n",
    "# 모델의 forward 함수를 체크포인트로 감싸기\n",
    "def forward_with_checkpoint(input):\n",
    "    return checkpoint(model, input)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 배치 크기 설정 (메모리 사용량에 따라 조정)\n",
    "batch_size = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        \n",
    "        for data in batch:\n",
    "            _id = data[\"id\"]\n",
    "            messages = data[\"messages\"]\n",
    "            len_choices = data[\"len_choices\"]\n",
    "            \n",
    "            try:\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # CPU에서 처리\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                outputs = forward_with_checkpoint(inputs)\n",
    "                logits = outputs.logits[:, -1].flatten()\n",
    "\n",
    "                target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "                probs = torch.nn.functional.softmax(\n",
    "                    torch.tensor(target_logit_list, dtype=torch.float32),\n",
    "                    dim=-1\n",
    "                ).numpy()\n",
    "\n",
    "                predict_value = pred_choices_map[np.argmax(probs)]\n",
    "                infer_results.append({\"id\": _id, \"answer\": predict_value})\n",
    "                \n",
    "                print(f\"Processed sample {_id}: predicted {predict_value}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # 메모리 정리\n",
    "                del inputs\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"Processed {len(infer_results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
