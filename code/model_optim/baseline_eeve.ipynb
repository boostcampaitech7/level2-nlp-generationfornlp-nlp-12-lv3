{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation for NLP Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, EarlyStoppingCallback, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 고정\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42) # magic number :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "dataset = pd.read_csv('../../data/train.csv') \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>question_plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-425</td>\n",
       "      <td>상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...</td>\n",
       "      <td>상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?</td>\n",
       "      <td>[ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-426</td>\n",
       "      <td>(가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...</td>\n",
       "      <td>(가)에 대한 설명으로 옳지 않은 것은?</td>\n",
       "      <td>[고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-427</td>\n",
       "      <td>나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...</td>\n",
       "      <td>(가) 지역에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-428</td>\n",
       "      <td>이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....</td>\n",
       "      <td>밑줄 친 ‘그’에 대한 설명으로 옳은 것은?</td>\n",
       "      <td>[살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-429</td>\n",
       "      <td>선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...</td>\n",
       "      <td>(가) 인물이 추진한 정책으로 옳지 않은 것은?</td>\n",
       "      <td>[사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                          paragraph  \\\n",
       "0  generation-for-nlp-425  상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服)...   \n",
       "1  generation-for-nlp-426  (가)은/는 의병계열과 애국계몽 운동 계열의 비밀결사가 모여 결성된 조직으로, 총사...   \n",
       "2  generation-for-nlp-427  나는 삼한(三韓) 산천의 음덕을 입어 대업을 이루었다.(가)는/은 수덕(水德)이 순...   \n",
       "3  generation-for-nlp-428  이 날 소정방이 부총관 김인문 등과 함께 기 벌포에 도착하여 백제 군사와 마주쳤다....   \n",
       "4  generation-for-nlp-429  선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가...   \n",
       "\n",
       "                                question  \\\n",
       "0  상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?   \n",
       "1                 (가)에 대한 설명으로 옳지 않은 것은?   \n",
       "2                 (가) 지역에 대한 설명으로 옳은 것은?   \n",
       "3               밑줄 친 ‘그’에 대한 설명으로 옳은 것은?   \n",
       "4             (가) 인물이 추진한 정책으로 옳지 않은 것은?   \n",
       "\n",
       "                                             choices  answer question_plus  \n",
       "0                           [ㄱ, ㄴ, ㄱ, ㄷ, ㄴ, ㄹ, ㄷ, ㄹ]       2          None  \n",
       "1  [고려 문종 때에 남경(南京)으로 승격되었다., 종루(鐘樓), 이현, 칠패 등에서 ...       1          None  \n",
       "2  [이곳에 대장도감을 설치하여 재조대장경을 만들었다., 지눌이 이곳에서 수선사 결사운...       4          None  \n",
       "3  [살수에서 수의 군대를 물리쳤다 ., 김춘추 의 신라 왕위 계승을 지원하였다 ., ...       2          None  \n",
       "4  [사창제를 실시하였다 ., 대전회통을 편찬하였다 ., 비변사의 기능을 강화하였다 ....       3          None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "id                  0\n",
      "paragraph           0\n",
      "question            0\n",
      "choices             0\n",
      "answer              0\n",
      "question_plus    2031\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2031 entries, 0 to 2030\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             2031 non-null   object\n",
      " 1   paragraph      2031 non-null   object\n",
      " 2   question       2031 non-null   object\n",
      " 3   choices        2031 non-null   object\n",
      " 4   answer         2031 non-null   int64 \n",
      " 5   question_plus  0 non-null      object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 95.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on 'question' and 'choices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'question' and 'question_plus' if available\n",
    "df['question_plus'] = df['question_plus'].fillna('')\n",
    "df['full_question'] = df.apply(lambda x: x['question'] + ' ' + x['question_plus'] if x['question_plus'] else x['question'], axis=1)\n",
    "\n",
    "# Calculate the length of each question\n",
    "df['question_length'] = df['full_question'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Question Lengths')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF\n",
    "\n",
    "- TF-IDF 참고 링크: https://ko.wikipedia.org/wiki/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transform the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_question'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTF-IDF Features:\")\n",
    "display(tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "- https://huggingface.co/beomi/gemma-ko-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `assignment_2_persona` has been saved to /data/ephemeral/home/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /data/ephemeral/home/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `assignment_2_persona`\n"
     ]
    }
   ],
   "source": [
    "# 본인의 Huggingface auth token 입력\n",
    "## Jupyter lab에서 로그인 하는 textbox가 나오지 않을 경우, terminal에서 로그인 하실 수 있습니다.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델과 토크나이저를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44.1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "print(bnb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66be90b4d01d43599ecd72e74e3f44bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NO_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
    "정답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'paragraph', 'question', 'choices', 'answer', 'question_plus'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 답을 말해주세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i][\"answer\"],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be37167c2284c9f8614586dda9545ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing (num_proc=4):   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=2048,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    ) \n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e780fd8fbb0466c92c9f60e82d0978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "지문을 읽고 답을 말해주세요. \n",
      " user\n",
      "지문:\n",
      "중국 스타벅스에서 카페라테 한 잔을 마시려면 미국에서보다 약 1달러 더 비싼 4.80달러(약 5237원)를 내야 한다. 미국에서 7만3000달러인 캐딜락 에스컬레이드는 중국에서 22만9000달러(약 2억5000만원), 애플 아이패드2는 중국에서 미국보다 100달러 더 비싼 488달러(약 53만원)다. 왜 이런 현상이 벌어질까.월스트리트저널(WSJ)은 중국 ‘살인 물가’의 원인이 높은 세금과 중국 중산층의 과시욕 때문이라고 4일(현지시간) 보도했다. 중국 중산층이 급격히 늘면서 수입품을 선호하는 소비자가 늘었고, 가격이 비쌀수록 더 잘 팔리는 중국 시장을 겨냥해 외국 브랜드들이 가격을 더 높게 책정해왔다는 것이다.스타벅스가 대표적이다. 미국에서는 대중적인 커피 전문점이지만 중국에선 고급 이미지로 자리 잡았다. 중간 크기의 아메리카노 한 잔 가격은 미국보다 75% 비싸게 팔린다. 유벌 애츠먼 맥킨지앤드컴퍼니 사장은 “중국 소비자는 가격이 비쌀수록 품질이 뛰어나고, 명품을 소비하면 자신도 명품이 된다고 생각하는 경향이 있다”며 “많은 외국계 기업이 이런 소비자의 성향을 겨냥해 많은 수익을 내왔다”고 말했다. 중국 정부의 복잡한 행정 절차가 소비자 물가를 올리는 원인이라는 지적도 있다. 법률회사 캐드월더러 워커셤앤드태프트의 중국 담당자인 로키 리는 “중국에서 매장 하나를 열기 위해선 여러 단계의 복잡한 행정 절차를 걸쳐 정부의 허가를 받아야 하기 때문에 적게는 몇 달에서 길게는 몇 년이 걸린다”며 “이런 비용이 모두 소비자 부담으로 이어지고 있는 것”이라고 지적했다.WSJ는 중국에서만 비싸게 받던 외국 기업들의 관행은 지속되기 힘들 것이라고 전망했다. 인터넷이 널리 보급되고 중국인의 해외 여행이 급증하면서 소비자가 쉽게 가격 비교를 하고 있기 때문이다.\n",
      "\n",
      "질문:\n",
      "중국에서 외국 브랜드가 가격을 높게 책정하는 주된 이유는 무엇인가?\n",
      "\n",
      "선택지:\n",
      "1 - 중국 정부의 복잡한 행정 절차\n",
      "2 - 중국 중산층의 과시욕\n",
      "3 - 중국의 낮은 세금\n",
      "4 - 중국 소비자의 가격 민감성\n",
      "5 - 중국의 높은 생산 비용\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "정답: \n",
      " assistant\n",
      "2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "# vram memory 제약으로 인해 인풋 데이터의 길이가 1024 초과인 데이터는 제외하였습니다. *힌트: 1024보다 길이가 더 긴 데이터를 포함하면 더 높은 점수를 달성할 수 있을 것 같습니다!\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 1024)\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.column_names)\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(train_dataset[1][\"input_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1024\n",
      "min token length: 115\n",
      "avg token length: 510.92414248021106\n"
     ]
    }
   ],
   "source": [
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion 부분만 학습하기 위한 data collator 설정\n",
    "\n",
    "- 텍스트 중 response_template 까지는 ignore_index 로 loss 계산에서 제외\n",
    "- 텍스트 중 response_template 이후는 학습에 포함 (정답 + eos 토큰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCollatorForCompletionOnlyLM(tokenizer=LlamaTokenizerFast(name_or_path='yanolja/EEVE-Korean-Instruct-10.8B-v1.0', vocab_size=40960, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "response_template = \"<|im_start|>assistant\\n\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ") \n",
    "\n",
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 logits 를 조정하여 정답 토큰 부분만 출력하도록 설정\n",
    "def preprocess_logits_for_metrics(logits, labels): \n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"], tokenizer.vocab[\"2\"], tokenizer.vocab[\"3\"], tokenizer.vocab[\"4\"], tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "# metric 로드\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# 정답 토큰 매핑\n",
    "int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "# <end_of_turn> 대신 </s>로 대체하여 정답만 남기고 나머지 제거\n",
    "def extract_answer_from_label(label):\n",
    "    return label.split()[-1]\n",
    "\n",
    "# metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "    \n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = list(map(lambda x: x.split(\"<|im_end|>\")[0].strip(), labels)) \n",
    "    labels = list(map(lambda x: int_output_map[x], labels))\n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그트 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '<|im_end|>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<|im_end|>'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad token 설정 \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# # GPU가 사용 가능한지 확인\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA를 사용할 수 없습니다. CPU로 모델을 학습합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length=4096,\n",
    "    output_dir=\"outputs_eeve\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-6,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ") \n",
    "\n",
    "seed = 42\n",
    "sampled_train_dataset = train_dataset.shuffle(seed=seed).select(range(10)) \n",
    "sampled_eval_dataset = eval_dataset.shuffle(seed=seed).select(range(10)) \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    "    # callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"transformer.h.0\" in name:  # 예: 초기 레이어를 동결\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # True 출력 확인\n",
    "print(torch.cuda.get_device_name(0))  # Tesla V100 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='1137' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  11/1137 00:34 < 1:11:14, 0.26 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 136.38 MiB is free. Process 3575394 has 31.60 GiB memory in use. Of the allocated memory 30.65 GiB is allocated by PyTorch, and 592.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:823\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:811\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    481\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    483\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 136.38 MiB is free. Process 3575394 has 31.60 GiB memory in use. Of the allocated memory 30.65 GiB is allocated by PyTorch, and 592.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from accelerate import Accelerator\n",
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from peft import PeftModel\n",
    "\n",
    "# TODO 학습된 Checkpoint 경로 입력\n",
    "checkpoint_path = \"../../data/outputs_eeve/checkpoint-15\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    checkpoint_path,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data 경로 입력\n",
    "test_df = pd.read_csv('../../data/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <보기>가 있을 때\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"지문을 읽고 질문의 답을 구하세요.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "\n",
    "sampled_test_dataset = test_dataset[:10]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(sampled_test_dataset):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        if isinstance(inputs, dict):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)  \n",
    "        else:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        logits = outputs.logits[:, -1].flatten().cpu()\n",
    "\n",
    "        target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "        probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(target_logit_list, dtype=torch.float32)\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        predict_value = pred_choices_map[np.argmax(probs, axis=-1)]\n",
    "        infer_results.append({\"id\": _id, \"answer\": predict_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "infer_results = []\n",
    "pred_choices_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).half()\n",
    "\n",
    "# 모델의 forward 함수를 체크포인트로 감싸기\n",
    "def forward_with_checkpoint(input):\n",
    "    return checkpoint(model, input)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 배치 크기 설정 (메모리 사용량에 따라 조정)\n",
    "batch_size = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i:i+batch_size]\n",
    "        \n",
    "        for data in batch:\n",
    "            _id = data[\"id\"]\n",
    "            messages = data[\"messages\"]\n",
    "            len_choices = data[\"len_choices\"]\n",
    "            \n",
    "            try:\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                # CPU에서 처리\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                outputs = forward_with_checkpoint(inputs)\n",
    "                logits = outputs.logits[:, -1].flatten()\n",
    "\n",
    "                target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]\n",
    "\n",
    "                probs = torch.nn.functional.softmax(\n",
    "                    torch.tensor(target_logit_list, dtype=torch.float32),\n",
    "                    dim=-1\n",
    "                ).numpy()\n",
    "\n",
    "                predict_value = pred_choices_map[np.argmax(probs)]\n",
    "                infer_results.append({\"id\": _id, \"answer\": predict_value})\n",
    "                \n",
    "                print(f\"Processed sample {_id}: predicted {predict_value}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # 메모리 정리\n",
    "                del inputs\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"Processed {len(infer_results)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results).to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(infer_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
