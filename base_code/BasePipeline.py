import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import random
import evaluate
from peft import AutoPeftModelForCausalLM
from datasets import Dataset
from ast import literal_eval
from utils.load import load_config
import config.prompts as config_prompts
from ModelManager import ModelManager 
from typing import Type, Tuple, Optional

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

class BasePipeline:
    def __init__(self, config_name: str, Manager: Type[ModelManager]):
        """BasePipeline 클래스의 생성자입니다.

        Args:
            config_name (str): config 파일의 경로 (확장자 제외)
        """
        config = load_config(config_name)
        
        self.config_name = config_name.split("/")[-1]
        
        self.data_config = config["data"]
        self.data_path = self.data_config["data_path"]
        
        self.experiment_config = config["experiment"]
        
        model_config = config["model"]
        params = config["params"]
        self.manager = Manager(model_config, params)
    
    def preprocess_logits_for_metrics(self, logits, labels):
        tokenizer = self.manager.tokenizer
        logits = logits if not isinstance(logits, tuple) else logits[0]
        logit_idx = [tokenizer.vocab["1"], tokenizer.vocab["2"], tokenizer.vocab["3"], tokenizer.vocab["4"], tokenizer.vocab["5"]]
        logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token
        return logits
    
    def load_dataset(self, dataset: pd.DataFrame) -> pd.DataFrame:
        """데이터프레임을 로드하여 필요한 필드를 추가하여 데이터프레임을 반환합니다.

        Args:
            dataset (pd.DataFrame): "id", "paragraph", "problems" 필드를 포함해야 합니다.

        Returns:
            pd.DataFrame: "id", "paragraph", "question", "choices", "answer" 필드를 포함한 데이터프레임을 반환합니다.
        """
        records = []
        for _, row in dataset.iterrows():
            problems = literal_eval(row['problems'])
            record = {
                'id': row['id'],
                'paragraph': row['paragraph'],
                'question': problems['question'],
                'choices': problems['choices'],
                'answer': problems.get('answer', None),
                "question_plus": problems.get('question_plus', None),
            }
            if 'question_plus' in problems:
                record['question_plus'] = problems['question_plus']
            records.append(record)
        df = pd.DataFrame(records)
        return df
    
    def _load_dataset(self, mode : str = "train", df : Optional[pd.DataFrame] = None) -> Dataset:
        """
        csv 파일을 로드하여 허깅페이스 Dataset 형태로 반환합니다.
        
        Args:
            mode (str): 데이터셋 모드 (train, dev, test 중 하나)
            df (pd.DataFrame): 데이터프레임을 직접 입력할 경우 사용합니다.
            
        Returns:
            Dataset: 데이터셋
        """
        
        file_name = None
        if df is not None:
            return Dataset.from_pandas(self.load_dataset(df))
        elif mode == "test":
            file_name = self.data_config["test_file"]
        elif mode == "dev":
            file_name = self.data_config["dev_file"]
        elif mode == "train":
            file_name = self.data_config["train_file"]
        else:
            raise ValueError("mode는 train 또는 test 중 하나여야 합니다.")    
    
        return Dataset.from_pandas(self.load_dataset(pd.read_csv(self.data_path + "/" + file_name)))
    
    def make_user_messages(self, row: dict) -> str:
        """데이터셋의 각 row를 이용하여 사용자 프롬프트를 생성합니다.

        Args:
            row (dict): "paragraph", "question", "choices", "question_plus" 필드를 포함해야 합니다.

        Returns:
            str: 사용자 프롬프트를 반환합니다.
        """
        choice_string = "\n".join([f"{idx + 1} - {choice}" for idx, choice in enumerate(row["choices"])])
        
        # <보기>가 있을 때
        if row["question_plus"]:
            user_message = config_prompts.PROMPT_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                question_plus=row["question_plus"],
                choices=choice_string,
            )
        # <보기>가 없을 때
        else:
            user_message = config_prompts.PROMPT_NO_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                choices=choice_string,
            )
        
        return user_message
    
    def make_chat_message(self, row: dict, user_message: str) -> dict:
        """데이터셋의 각 row와 그 row로부터 얻은 사용자 프롬프트를 이용하여 챗봇 프롬프트를 포함하는 딕셔너리를 생성합니다.
        상속받는 클래스에서 이 메소드를 오버라이드할 것.

        Args:
            row (dict): "id", "answer" 필드를 포함해야 합니다.
            user_message (str): 사용자 프롬프트 

        Returns:
            dict: 챗봇 프롬프트를 포함한 딕셔너리. Dataset으로 변환됩니다.
        """
        return {
            "id": row["id"],
            "messages": [
                {"role": "system", "content": "지문을 읽고 질문의 답을 구하세요."},
                {"role": "user", "content": user_message},
            ],
            "label": row["answer"],
        }
    
    def _make_chat_message(self, row: dict, user_message: str, mode="train") -> dict:
        """데이터셋의 각 row와 그 row로부터 얻은 사용자 프롬프트를 이용하여 챗봇 프롬프트를 포함하는 딕셔너리를 생성합니다.
        데이터셋 모드에 따라 챗봇 프롬프트에 정답을 포함할지 여부를 결정합니다.
        
        Args:
            row (dict): "id", "answer", "choices" 필드를 포함해야 합니다.
            user_message (str): 사용자 프롬프트 
            mode (str): 데이터셋 모드 (train, test 중 하나)

        Returns:
            dict: 챗봇 프롬프트를 포함한 딕셔너리. Dataset으로 변환됩니다.
        """
        chat_message = self.make_chat_message(row, user_message)
        if mode == "train":
            chat_message["messages"].append({"role": "assistant", "content": f"{chat_message['label']}"})
        elif mode == "test":
            chat_message["len_choices"] = len(row["choices"])
        else:
            raise ValueError("mode는 train 또는 test 중 하나여야 합니다.")
        
        return chat_message    
    
    def process_dataset(self, dataset: Dataset, mode="train") -> Dataset:
        """입력으로 들어온 dataset을 이용하여 입력 프롬프트와 그에 대한 정답을 생성합니다.

        Args:
            dataset (Dataset): 입력으로 들어온 dataset. 아래 필드를 포함하고 있어야 합니다.
            (id-식별자, 지문-paragraph, 질문-question, 선지-choices, 보기-question_plus,정답-answer)
            mode (str): 데이터셋 모드 (train, test 중 하나)
        
        Returns:
            Dataset: 아래 필드를 포함하고 있는 dataset을 반환합니다.
            (id-식별자, messages-프롬프트를 위한 message, label-정답)
        """
        processed_dataset = []
        for i in range(len(dataset)):
            user_message = self.make_user_messages(dataset[i])

            if len(dataset[i]["choices"]) == 4:
                user_message = user_message.replace("1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.", "1, 2, 3, 4 중에 하나를 정답으로 고르세요.")

            # chat message 형식으로 변환
            processed_dataset.append(
                self._make_chat_message(dataset[i], user_message, mode=mode)
            )
        
        return Dataset.from_pandas(pd.DataFrame(processed_dataset))
    
    def compute_metrics(self, evaluation_result):
        int_output_map = {"1": 0, "2": 1, "3": 2, "4": 3, "5": 4}
        acc_metric = evaluate.load("accuracy")

        logits, labels = evaluation_result
        tokenizer = self.manager.tokenizer
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        labels = list(map(lambda x: x.split("<end_of_turn")[0].strip(), labels))
        labels = list(map(lambda x: int_output_map[x], labels))

        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)
        predictions = np.argmax(probs, axis=-1)

        acc = acc_metric.compute(predictions=predictions, references=labels)
        
        return acc

    def report_metrics(self, metrics):
        print("-" * 30)
        print("Evaluation Accuracy : {:.4f}".format(metrics["eval_accuracy"]))
        print("-" * 30)

    def get_train_and_valid_df(self, eval_dataset: Optional[Dataset]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        dev_file = self.data_config.get("dev_file", None)
        if dev_file is not None:
            # dev_file을 직접 지정한 경우, 이 경우에는 이 함수를 호출하지 않는 것을 추천합니다.
            return (
                pd.read_csv(self.data_path + self.data_config["train_file"]),
                pd.read_csv(self.data_path + dev_file)
            )
        else: # splited
            all_df = pd.read_csv(self.data_path + self.data_config["train_file"])
            valid_df_ids = set(eval_dataset["id"]) if eval_dataset is not None else set()
            train_df = all_df[~all_df["id"].isin(valid_df_ids)]
            eval_df = all_df[all_df["id"].isin(valid_df_ids)]
            return train_df, eval_df

    def train(self):
        # train task
        dataset = self._load_dataset()
        
        if self.manager.model is None:
            self.manager.set_model()
        if self.manager.tokenizer is None:
            self.manager.set_tokenizer()
        
        processed_dataset = self.process_dataset(dataset)

        tokenized_dataset = processed_dataset.map(
            self.manager.apply_chat_template_and_tokenize,
            remove_columns=list(processed_dataset.features),
            batched=True,
            num_proc=self.data_config.get("tokenizer_num_procs", 1),
            load_from_cache_file=True,
            desc="Tokenizing",
        )

        # 데이터 분리
        # 데이터 길이 default = 1024
        filter_len = self.data_config.get("filtering_input_ids_length", 1024)
        if filter_len > 0:
            tokenized_dataset = tokenized_dataset.filter(lambda x: len(x["input_ids"]) <= filter_len)  
        
        # 데이터셋 분리 (test_size 설정과 dev_file 설정 고려)
        test_size = float(self.data_config.get("test_size", 0.1))
        dev_file = self.data_config.get("dev_file", None)
        if test_size > 0 and dev_file is not None:
            tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)

            train_dataset = tokenized_dataset['train']
            eval_dataset = tokenized_dataset['test']
        elif dev_file is not None: 
            train_dataset = tokenized_dataset
            
            processed_dev_dataset = self.process_dataset(self._load_dataset(mode="dev"))
            tokenized_dev_dataset = processed_dev_dataset.map(
                self.manager.apply_chat_template_and_tokenize,
                remove_columns=list(processed_dataset.features),
                batched=True,
                num_proc=self.data_config.get("tokenizer_num_procs", 1),
                load_from_cache_file=True,
                desc="Tokenizing",
            )
            
            eval_dataset = tokenized_dev_dataset
        else:
            train_dataset = tokenized_dataset
            eval_dataset = None

        # save train and eval dataset
        train_df, eval_df = self.get_train_and_valid_df(eval_dataset)
        if self.experiment_config.get("save_train_dataset", False):
            train_df.to_csv(f"{self.config_name}-train.csv", index=False)
        if self.experiment_config.get("save_eval_dataset", False):
            eval_df.to_csv(f"{self.config_name}-eval.csv", index=False)

        if self.manager.data_collator is None:
            self.manager.set_data_collator()
        
        if self.manager.trainer is None:
            self.manager.set_trainer(train_dataset, eval_dataset, self.compute_metrics, self.preprocess_logits_for_metrics)
        
        self.manager.train()
        
        # 훈련 종료시 evaluate를 진행할지 안할지 결정
        last_eval_strategy = self.experiment_config.get("last_eval_strategy", "no")
        if (test_size > 0 or dev_file is not None) and not last_eval_strategy == "no":
            if last_eval_strategy == "evaluate":
                final_metrics = self.manager.evaluate()
                self.report_metrics(final_metrics)
            elif last_eval_strategy == "inference":
                _, eval_df = self.get_train_and_valid_df(eval_dataset)
                processed_df = self.process_dataset(self._load_dataset(df=eval_df), mode="test")
                output = self.do_inference(self.manager.model, processed_df)
                output.to_csv(f"{self.config_name}-eval-output.csv", index=False)
        
    def do_inference(self, model: AutoPeftModelForCausalLM, dataset: Dataset) -> pd.DataFrame:
        infer_results = []
        pred_choices_map = {0: "1", 1: "2", 2: "3", 3: "4", 4: "5"}
        tokenizer = self.manager.tokenizer
        model.eval()
        with torch.inference_mode():
            for data in tqdm(dataset):
                _id = data["id"]
                messages = data["messages"]
                len_choices = data["len_choices"]

                outputs = model(
                    tokenizer.apply_chat_template(
                        messages,
                        tokenize=True,
                        add_generation_prompt=True,
                        return_tensors="pt",
                    ).to(DEVICE)
                )

                logits = outputs.logits[:, -1].flatten().cpu()

                target_logit_list = [logits[tokenizer.vocab[str(i + 1)]] for i in range(len_choices)]

                probs = (
                    torch.nn.functional.softmax(
                        torch.tensor(target_logit_list, dtype=torch.float32)
                    )
                    .detach()
                    .cpu()
                    .numpy()
                )

                predict_value = pred_choices_map[np.argmax(probs, axis=-1)]
                infer_results.append({"id": _id, "answer": predict_value})
                
        return pd.DataFrame(infer_results)
    
    def inference(self):
        dataset = self._load_dataset(mode="test")
        if self.manager.model is None:
            self.manager.set_model(AutoModel=AutoPeftModelForCausalLM)
        elif self.manager.trainer.state.best_model_checkpoint is not None:
            original_model_name = self.manager.model_name_or_checkpoint
            self.manager.model_name_or_checkpoint = self.manager.trainer.state.best_model_checkpoint
            self.manager.set_model(AutoModel=AutoPeftModelForCausalLM)
            self.manager.model_name_or_checkpoint = original_model_name
        elif self.manager.trainer is not None:
            self.manager.model = self.manager.trainer.model
            
        if self.manager.tokenizer is None:
            self.manager.set_tokenizer()

        test_dataset = self.process_dataset(dataset, mode="test")
        output = self.do_inference(self.manager.model, test_dataset)
        output.to_csv("output.csv", index=False)
        print("Successfully saved the output csv file!")
        